{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression\n",
    "\n",
    "Watch the 9-minute video below for a visual explanation of simple linear regression as a line-fitting problem.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/PaFPbb66DxQ?start=24\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Explaining Linear Regression, by StatQuest](https://www.youtube.com/embed/PaFPbb66DxQ?start=24)\n",
    "```\n",
    "\n",
    "\n",
    "Then study the following sections to learn more about simple linear regression with examples in the text book.\n",
    "\n",
    "### Import libraries and load data\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries.\n",
    "<!-- Firstly, we will import the required libraries and load the `Advertising` dataset. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [Advertising dataset](https://github.com/pykale/transparentML/raw/main/data/Advertising.csv) and display the column names, counts and data types.\n",
    "\n",
    "<!-- Datasets available on https://www.statlearning.com/resources-first-edition -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Advertising.csv\"\n",
    "\n",
    "advertising_df = pd.read_csv(data_url, header=0, index_col=0)\n",
    "advertising_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 5 rows for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertising_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression assumes that there is an approximately linear relationship between a quantitative response $y$ and a single predictor $x$. Mathematically, the linear relationship can be expressed as\n",
    "\n",
    "\\begin{equation}\n",
    "y \\approx \\beta_0 + \\beta_1 x\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are two unknown constants that represent the *weight* and *bias* of the linear model, which are also known as *intercept* and *slope*. Together, $\\beta_0$ and $\\beta_1$ are called the *model coefficients*, or *parameters*. We can describe the linear relationship as *regressing* $y$ onto $x$. For example, $x$ may represent `TV` advertising and $y$ may represent `sales`. Then we can regress sales onto TV by fitting the model\n",
    "\n",
    "<!-- \\begin{equation} -->\n",
    "$$\n",
    "\\textrm{sales} \\approx \\beta_0 + \\beta_1 \\times \\textrm{TV}\n",
    "$$\n",
    "<!-- \\end{equation} -->\n",
    "\n",
    "\n",
    "\n",
    "### Estimating the coefficients\n",
    "\n",
    "The goal of simple linear regression is to estimate the unknown parameters $\\beta_0$ and $\\beta_1$ from the data. Let \n",
    "\n",
    "$$\n",
    "(x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N)\n",
    "$$\n",
    "\n",
    "be the $n$ observations in the dataset, and $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ denote the estimated values of $\\beta_0$ and $\\beta_1$, respectively. The estimated values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ can be obtained by minimising the *residual sum of squares* (RSS)\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS} = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^N (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $y_i$ is the $i\\text{th}$ observation of the response $y$, $\\hat{y}_i$ is the $i\\text{th}$ observation of the predicted response $\\hat{Y}$, and $x_i$ is the $i\\text{th}$ observation of the predictor $x$. The least squares estimates of $\\beta_0$ and $\\beta_1$ are given by\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^N (x_i - \\bar{x})^2},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of $x$ and $y$ respectively. The least squares estimates of $\\beta_0$ and $\\beta_1$ are obtained by minimising the RSS. The least squares line is given by\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n",
    "\\end{equation}\n",
    "\n",
    "The least squares line is the line that minimises the sum of squared residuals. The least squares line is also known as the *regression line*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to fit `sales` onto `TV` for the `Advertising` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(\n",
    "    x=advertising_df.TV,\n",
    "    y=advertising_df.Sales,\n",
    "    order=1,\n",
    "    ci=None,\n",
    "    scatter_kws={\"color\": \"r\", \"s\": 9},\n",
    ")\n",
    "plt.xlim(-10, 310)\n",
    "plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the least squares fit of `sales` onto `TV` for the `Advertising` dataset. The objective is to minimise the sum of squared residuals, which is the sum of the vertical distances between the data points (red dots) and the least squares line (blue line). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: fitting a linear regression model and visualising regression coefficients using `scikit-learn`**\n",
    "\n",
    "The `LinearRegression` class in `scikit-learn` is used to fit a linear regression model. The `.fit()` method takes two arguments, the first is the predictor variable and the second is the response variable. The `.fit()` method returns an object that contains the estimated coefficients. The `.intercept_` and `.coef_` attributes of the fitted model can be used to obtain the estimated intercept ($\\beta_0$) and slope ($\\beta_1$) of the regression line. \n",
    "<!-- \n",
    "Note that the text in the book describes the coefficients based on unnormalised data, whereas the plot shows the model based on normalised data. The latter is visually more appealing for explaining the concept of a minimum RSS. I think that, in order not to confuse the reader, the values on the axis of the `beta_0` coefficients have been changed to correspond with the text. The axes on the plots below are unaltered. -->\n",
    "\n",
    "Firstly, fit a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression coefficients (Ordinary Least Squares)\n",
    "regr = LinearRegression()\n",
    "\n",
    "X = scale(advertising_df.TV, with_mean=True, with_std=False).reshape(-1, 1)\n",
    "y = advertising_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(regr.intercept_)\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create grid coordinates for plotting and compute the minimum of RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.linspace(regr.intercept_ - 2, regr.intercept_ + 2, 50)\n",
    "beta_1 = np.linspace(regr.coef_ - 0.02, regr.coef_ + 0.02, 50)\n",
    "xx, yy = np.meshgrid(beta_0, beta_1, indexing=\"xy\")\n",
    "Z = np.zeros((beta_0.size, beta_1.size))\n",
    "\n",
    "# Calculate Z-values (RSS) based on grid of coefficients\n",
    "for (i, j), v in np.ndenumerate(Z):\n",
    "    Z[i, j] = ((y - (xx[i, j] + X.ravel() * yy[i, j])) ** 2).sum() / 1000\n",
    "\n",
    "# minimised RSS\n",
    "min_RSS = r\"$\\beta_0$, $\\beta_1$ for minimised RSS\"\n",
    "min_rss = (\n",
    "    np.sum((regr.intercept_ + regr.coef_ * X - y.values.reshape(-1, 1)) ** 2) / 1000\n",
    ")\n",
    "min_rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the RSS with corresponding $\\beta_0$ and $\\beta_1$ in 2D and 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "fig.suptitle(\"RSS - Regression coefficients\", fontsize=20)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "\n",
    "# Left plot\n",
    "CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3])\n",
    "ax1.scatter(regr.intercept_, regr.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax1.clabel(CS, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "\n",
    "# Right plot\n",
    "ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3)\n",
    "ax2.contour(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    zdir=\"z\",\n",
    "    offset=Z.min(),\n",
    "    cmap=plt.cm.Set1,\n",
    "    alpha=0.4,\n",
    "    levels=[2.15, 2.2, 2.3, 2.5, 3],\n",
    ")\n",
    "ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=\"r\", label=min_RSS)\n",
    "ax2.set_zlabel(\"RSS\")\n",
    "ax2.set_zlim(Z.min(), Z.max())\n",
    "ax2.set_ylim(0.02, 0.07)\n",
    "\n",
    "# settings common to both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=17)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=17)\n",
    "    ax.set_yticks([0.03, 0.04, 0.05, 0.06])\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the accuracy of the model\n",
    "\n",
    "The accuracy of the linear model is dependent on the variability of the response $y$ and the predictor $x$. The variability of $y$ is measured by the variance of $y$, denoted by $\\sigma^2$. The variability of $x$ is measured by the variance of $x$, denoted by $\\sigma_x^2$. The coefficient of determination, denoted by $R^2$, is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{TSS} = \\sum_{i=1}^N (y_i - \\bar{y})^2$ *is the total sum of squares*. Dividing the TSS by the total number of training samples gives the *mean squared error* (MSE) of the model. The MSE is the average squared distance between the observed response values and the response values predicted by the model. The MSE is also known as the *mean squared prediction error* (MSPE). The MSE is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MSE} = \\frac{1}{N} \\text{TSS} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "The coefficient of determination is a proportion that measures the proportion of the variability in the response that is explained by the linear model. The coefficient of determination is always between 0 and 1. The coefficient of determination is 0 when the regression line does not fit the data at all, and the coefficient of determination is 1 when the regression line perfectly fits the data. The coefficient of determination is also known as the *coefficient of multiple determination*. The coefficient of determination is a measure of the goodness of fit of the linear model. The coefficient of determination is also known as the *coefficient of multiple determination*. The coefficient of determination is a measure of the goodness of fit of the linear model.\n",
    "\n",
    "Watch the video below to learn more about $R^2$\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/2AQKmw14mHM?start=16\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Explaining $R^2$, by StatQuest](https://www.youtube.com/embed/2AQKmw14mHM?start=16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to fit a simple linear regression model, and then evaluate the learnt model with $R^2$ using `scikit-learn` (Table 3.1 & 3.2 of the text book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "X = advertising_df.TV.values.reshape(-1, 1)\n",
    "y = advertising_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(regr.intercept_)\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_pred = regr.predict(X)\n",
    "print(\"R2 score:\", r2_score(y, sales_pred))\n",
    "print(\"Mean squared error: \", mean_squared_error(y, sales_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the accuracy of the coefficient estimates (Advanced)\n",
    "\n",
    "The linear relationship between $x$ and $y$ can be written in an equation as\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\epsilon$ is a random error term that represents the difference between the observed response $y$ and the true response $\\beta_0 + \\beta_1 X$. The error term $\\epsilon$ is assumed to be normally distributed with mean zero and constant variance $\\sigma^2$. The coefficient estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are only estimates of the true coefficients $\\beta_0$ and $\\beta_1$. We can quantify the accuracy of the estimates by computing the *standard error* of the estimates. The following formulars can be used to compute the standard error associated with $\\hat{\\beta}_1$ and $\\hat{\\beta}_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{SE}(\\hat{\\beta}_{1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^N (x_i - \\bar{x})^2}, \\quad \\text{SE}(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[\\frac{1}{N} + \\frac{\\bar{x}^2}{\\sum_{i=1}^N (x_i - \\bar{x})^2} \\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma^2$ is an estimate of the variance of the error term $\\epsilon= y - (\\beta_0 + \\beta_1 x) $, $\\hat{y}_i$ is the $i\\text{th}$ observation of the predicted response $\\hat{y}$, and $x_i$ is the $i\\text{th}$ observation of the predictor $x$, $\\bar{x}$ and $\\bar{y}$ are the sample means of $x$ and $y$ respectively. This estimate is called the *residual standard error* (RSE)\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSE} = \\sqrt{\\frac{1}{N-2}\\text{RSS}} = \\sqrt{\\frac{1}{N-2} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The standard error of $\\hat{\\beta}_1$ is a measure of the variability of the estimate of $\\beta_1$ due to sampling error. The standard error of $\\hat{\\beta}_0$ is a measure of the variability of the estimate of $\\beta_0$ due to sampling error. The standard error of $\\hat{\\beta}_1$ is also known as the *standard error of the regression*.\n",
    "\n",
    "Standard errors can be used to compute confidence intervals. A 95\\% is defined as a range of values such that with 95 \\% probability. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95 % confidence interval for $\\beta_1$ approximately takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_1 \\pm 2 \\times \\text{SE}(\\hat{\\beta}_1).\n",
    "\\end{equation}\n",
    "\n",
    "Run the following code to compute the statistics, including the confidence intervals, of the learnt model using `statesmodels` (page 67 & Table 3.1 & 3.2 of the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ TV\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS with regression coefficients\n",
    "(\n",
    "    (advertising_df.Sales - (est.params[0] + est.params[1] * advertising_df.TV)) ** 2\n",
    ").sum() / 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1bda8b49161908208a6e3087b59be7e64a5f9b1866581a6eaef662e2ef65b61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
