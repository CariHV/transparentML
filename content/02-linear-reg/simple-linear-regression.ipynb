{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression\n",
    "\n",
    "Firstly, we will import the required libraries and load the `Advertising` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**\n",
    "\n",
    "Datasets available on https://www.statlearning.com/resources-first-edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Advertising.csv\"\n",
    "\n",
    "advertising_df = pd.read_csv(\n",
    "    data_url, header=0, index_col=0\n",
    ")  # read Boston data as pandas data frame\n",
    "advertising_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertising_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression assumes that there is a approximately a linear relationship between a quantitative response $Y$ and a single predictor $X$. Mathematically, the linear relationship can be expressed as\n",
    "\n",
    "\\begin{equation}\n",
    "Y \\approx \\beta_0 + \\beta_1 X\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are two unknown constants that represent the *intercept* and *slope* of the linear model. Together, $\\beta_0$ and $\\beta_1$ are called the *model coefficients*, or *parameters*. We can describe the linear relationship as *regressing* $Y$ onto $X$. For example, $X$ may represent `TV` advertising and $Y$ may represent `sales`. Then we can regress sales onto TV by fitting the model\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{sales} \\approx \\beta_0 + \\beta_1 \\times \\textrm{TV}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "### Estimating the coefficients\n",
    "\n",
    "The goal of simple linear regression is to estimate the unknown parameters $\\beta_0$ and $\\beta_1$ from the data. Let \n",
    "\n",
    "$$\n",
    "(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\n",
    "$$\n",
    "\n",
    "be the $n$ observations in the dataset, and $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ denote the estimated values of $\\beta_0$ and $\\beta_1$, respectively. The estimated values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ can be obtained by minimizing the *residual sum of squares* (RSS)\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $y_i$ is the $i\\text{th}$ observation of the response $Y$, $\\hat{y}_i$ is the $i\\text{th}$ observation of the predicted response $\\hat{Y}$, and $x_i$ is the $i\\text{th}$ observation of the predictor $X$. The least squares estimates of $\\beta_0$ and $\\beta_1$ are given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of $X$ and $Y$ respectively. The least squares estimates of $\\beta_0$ and $\\beta_1$ are obtained by minimizing the RSS. The least squares line is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n",
    "\\end{equation}\n",
    "\n",
    "The least squares line is the line that minimizes the sum of squared residuals. The least squares line is also known as the *regression line*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3.1 - Least squares fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(\n",
    "    x=advertising_df.TV,\n",
    "    y=advertising_df.Sales,\n",
    "    order=1,\n",
    "    ci=None,\n",
    "    scatter_kws={\"color\": \"r\", \"s\": 9},\n",
    ")\n",
    "plt.xlim(-10, 310)\n",
    "plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3.2 - Regression coefficients - RSS**\n",
    "\n",
    "Note that the text in the book describes the coefficients based on uncentered data, whereas the plot shows the model based on centered data. The latter is visually more appealing for explaining the concept of a minimum RSS. I think that, in order not to confuse the reader, the values on the axis of the B0 coefficients have been changed to correspond with the text. The axes on the plots below are unaltered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression coefficients (Ordinary Least Squares)\n",
    "regr = LinearRegression()\n",
    "\n",
    "X = scale(advertising_df.TV, with_mean=True, with_std=False).reshape(-1, 1)\n",
    "y = advertising_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(regr.intercept_)\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid coordinates for plotting\n",
    "B0 = np.linspace(regr.intercept_ - 2, regr.intercept_ + 2, 50)\n",
    "B1 = np.linspace(regr.coef_ - 0.02, regr.coef_ + 0.02, 50)\n",
    "xx, yy = np.meshgrid(B0, B1, indexing=\"xy\")\n",
    "Z = np.zeros((B0.size, B1.size))\n",
    "\n",
    "# Calculate Z-values (RSS) based on grid of coefficients\n",
    "for (i, j), v in np.ndenumerate(Z):\n",
    "    Z[i, j] = ((y - (xx[i, j] + X.ravel() * yy[i, j])) ** 2).sum() / 1000\n",
    "\n",
    "# Minimized RSS\n",
    "min_RSS = r\"$\\beta_0$, $\\beta_1$ for minimized RSS\"\n",
    "min_rss = (\n",
    "    np.sum((regr.intercept_ + regr.coef_ * X - y.values.reshape(-1, 1)) ** 2) / 1000\n",
    ")\n",
    "min_rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "fig.suptitle(\"RSS - Regression coefficients\", fontsize=20)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "\n",
    "# Left plot\n",
    "CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3])\n",
    "ax1.scatter(regr.intercept_, regr.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax1.clabel(CS, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "\n",
    "# Right plot\n",
    "ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3)\n",
    "ax2.contour(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    zdir=\"z\",\n",
    "    offset=Z.min(),\n",
    "    cmap=plt.cm.Set1,\n",
    "    alpha=0.4,\n",
    "    levels=[2.15, 2.2, 2.3, 2.5, 3],\n",
    ")\n",
    "ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=\"r\", label=min_RSS)\n",
    "ax2.set_zlabel(\"RSS\")\n",
    "ax2.set_zlim(Z.min(), Z.max())\n",
    "ax2.set_ylim(0.02, 0.07)\n",
    "\n",
    "# settings common to both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=17)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=17)\n",
    "    ax.set_yticks([0.03, 0.04, 0.05, 0.06])\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the accuracy of the coefficient estimates\n",
    "\n",
    "The coefficient estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are only estimates of the true coefficients $\\beta_0$ and $\\beta_1$. We can quantify the accuracy of the estimates by computing the *standard error* of the estimates. The standard error of $\\hat{\\beta}_1$ is given by \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2(n - 2)}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}_i$ is the $i\\text{th}$ observation of the predicted response $\\hat{Y}$, and $x_i$ is the $i\\text{th}$ observation of the predictor $X$. The standard error of $\\hat{\\beta}_0$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{SE}(\\hat{\\beta}_0) = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n \\sum_{i=1}^n (x_i - \\bar{x})^2}}.\n",
    "\\end{equation}\n",
    "\n",
    "The standard error of $\\hat{\\beta}_1$ is a measure of the variability of the estimate of $\\beta_1$ due to sampling error. The standard error of $\\hat{\\beta}_0$ is a measure of the variability of the estimate of $\\beta_0$ due to sampling error. The standard error of $\\hat{\\beta}_1$ is also known as the *standard error of the regression*.\n",
    "\n",
    "\n",
    "\n",
    "### Assessing the accuracy of the model\n",
    "\n",
    "The accuracy of the linear model is dependent on the variability of the response $Y$ and the predictor $X$. The variability of $Y$ is measured by the variance of $Y$, denoted by $\\sigma^2$. The variability of $X$ is measured by the variance of $X$, denoted by $\\sigma_x^2$. The coefficient of determination, denoted by $R^2$, is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2$ is the total sum of squares. The coefficient of determination is a proportion that measures the proportion of the variability in the response that is explained by the linear model. The coefficient of determination is always between 0 and 1. The coefficient of determination is 0 when the regression line does not fit the data at all, and the coefficient of determination is 1 when the regression line perfectly fits the data. The coefficient of determination is also known as the *coefficient of multiple determination*. The coefficient of determination is a measure of the goodness of fit of the linear model. The coefficient of determination is also known as the *coefficient of multiple determination*. The coefficient of determination is a measure of the goodness of fit of the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence interval on page 67 & Table 3.1 & 3.2 - Statsmodels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ TV\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS with regression coefficients\n",
    "(\n",
    "    (advertising_df.Sales - (est.params[0] + est.params[1] * advertising_df.TV)) ** 2\n",
    ").sum() / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 3.1 & 3.2 - Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "X = advertising_df.TV.values.reshape(-1, 1)\n",
    "y = advertising_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(regr.intercept_)\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_pred = regr.predict(X)\n",
    "r2_score(y, Sales_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pykale')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f43f1712c352715dc9336599392a4cb54c95800761e3fc37b82a93ca73645ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
