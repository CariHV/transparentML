{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple linear regression\n",
    "\n",
    "Watch the 9-minute video below for a visual explanation of simple linear regression as a line-fitting problem.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/PaFPbb66DxQ?start=24\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Explaining Linear Regression, by StatQuest](https://www.youtube.com/embed/PaFPbb66DxQ?start=24)\n",
    "```\n",
    "\n",
    "\n",
    "Then study the following sections to learn more about simple linear regression with examples in the textbook.\n",
    "\n",
    "## Install libraries\n",
    "\n",
    "If you are using Google Colab, you can skip this section. If you are using your own computer, you will need to install the following libraries unless they are already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and load data\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries.\n",
    "<!-- Firstly, we will import the required libraries and load the `Advertising` dataset. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [Advertising dataset](https://github.com/pykale/transparentML/blob/main/data/Advertising.csv) and display the column names, counts and data types.\n",
    "\n",
    "<!-- Datasets available on https://www.statlearning.com/resources-first-edition -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Advertising.csv\"\n",
    "\n",
    "advertising_df = pd.read_csv(data_url, header=0, index_col=0)\n",
    "advertising_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 5 rows for inspection. You can also click [Advertising dataset](https://github.com/pykale/transparentML/blob/main/data/Advertising.csv) to inspect the full data on GitHub, since this dataset is small. It is a good habit to inspect the data before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertising_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear relationship modelling for regression\n",
    "\n",
    "Simple linear regression assumes that there is an approximately _linear_ relationship between a quantitative response  (output) $y$ and a single predictor (input) $x$. Mathematically, the linear relationship can be expressed as\n",
    "\n",
    "```{math}\n",
    ":label: y_approx_x\n",
    "y \\approx \\beta_0 + \\beta_1 x\n",
    "```\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are two unknown constants that represent the *bias* and *weight* of this linear model, which are also known as *intercept* and *slope*, respectively. Together, $\\beta_0$ and $\\beta_1$ are called the *model coefficients*, or *parameters*. We can describe the linear relationship as *regressing* $y$ onto $x$. For example, $x$ may represent `TV` advertising budget (in thousands of dollars) and $y$ may represent `sales` (in thousands of dollars) in the [Advertising dataset](https://github.com/pykale/transparentML/blob/main/data/Advertising.csv). Then we can regress sales onto TV by fitting the model:\n",
    "\n",
    "<!-- \\begin{equation} -->\n",
    "$$\n",
    "\\textrm{sales} \\approx \\beta_0 + \\beta_1 \\times \\textrm{TV}\n",
    "$$\n",
    "<!-- \\end{equation} -->\n",
    "\n",
    "\n",
    "\n",
    "## Estimating the coefficients\n",
    "\n",
    "The goal of simple linear regression is to estimate the unknown parameters $\\beta_0$ and $\\beta_1$ from the data. Let \n",
    "\n",
    "$$\n",
    "(x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N)\n",
    "$$\n",
    "\n",
    "be the $N$ observations in the dataset, and $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ denote the estimated values of $\\beta_0$ and $\\beta_1$, respectively. The estimated values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ can be obtained by minimising the [*residual sum of squares* (RSS)](https://en.wikipedia.org/wiki/Residual_sum_of_squares) between the observed response values $y_i$ and the predicted response values $\\hat{y}_i$:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS} = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^N (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $y_i$ is the $i\\text{th}$ observation of the response $y$, $\\hat{y}_i$ is the $i\\text{th}$ observation of the predicted response $\\hat{y}$, and $x_i$ is the $i\\text{th}$ observation of the predictor $x$. The least squares estimates of $\\beta_0$ and $\\beta_1$ are given as a [closed-form solution](https://mathworld.wolfram.com/Closed-FormSolution.html) by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^N (x_i - \\bar{x})^2},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of $x$ and $y$ respectively. The least squares estimates of $\\beta_0$ and $\\beta_1$ are obtained by minimising the RSS. The least squares line is given by\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x.\n",
    "\\end{equation}\n",
    "\n",
    "The least squares line, also known as the *regression line*, is the straight line that minimises the sum of squared residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to fit `sales` onto `TV` for the `Advertising` dataset using the API [`seaborn.regplot`](https://seaborn.pydata.org/generated/seaborn.regplot.html) from the library `seaborn`, which plots data and a linear regression model fit. You are encouraged to click the hyperlink to the API to learn more about its usage. The data are shown as red dots and the regression line is shown in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(\n",
    "    x=advertising_df.TV,\n",
    "    y=advertising_df.Sales,\n",
    "    order=1,\n",
    "    x_ci=\"ci\",\n",
    "    scatter_kws={\"color\": \"r\", \"s\": 9},\n",
    ")\n",
    "plt.xlim(-10, 310)\n",
    "plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a figure generated. This figure shows the least squares fit of `sales` onto `TV` for the `Advertising` dataset. The objective is to minimise the sum of squared residuals, which is the sum of the squared vertical distances, i.e. the _errors_, between the data points (red dots) and the least squares line (blue line). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Example: fitting a linear regression model and visualising regression coefficients using `scikit-learn`** -->\n",
    "## Example: model fitting and visualisation using `scikit-learn`\n",
    "\n",
    "Now we use the [`LinearRegression` class in `scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to fit a linear regression model. The [`.fit()` method](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) takes two arguments, the first is the predictor variable and the second is the response variable. The `.fit()` method returns an object that contains the estimated coefficients. The `.intercept_` and `.coef_` attributes of the fitted model can be used to obtain the estimated intercept ($\\beta_0$) and slope ($\\beta_1$) of the regression line. \n",
    "\n",
    "<!-- \n",
    "Note that the text in the book describes the coefficients based on unnormalised data, whereas the plot shows the model based on normalised data. The latter is visually more appealing for explaining the concept of a minimum RSS. I think that, in order not to confuse the reader, the values on the axis of the `beta_0` coefficients have been changed to correspond with the text. The axes on the plots below are unaltered. -->\n",
    "\n",
    "Firstly, fit a linear regression model. Before fitting, we centre the data by subtracting the mean of each variable from each observation via a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). This is a common practice in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression coefficients (Ordinary Least Squares)\n",
    "regr = LinearRegression()\n",
    "\n",
    "scale = StandardScaler(with_mean=True, with_std=False)\n",
    "X = scale.fit_transform(advertising_df.TV.values.reshape(-1, 1))\n",
    "y = advertising_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(\"Mean removed from input features: \", scale.mean_)\n",
    "print(\"Regression model intercept (bias): \", regr.intercept_)\n",
    "print(\"Regression model slop (weight): \", regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create grid coordinates for plotting and compute the minimum of RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.linspace(regr.intercept_ - 2, regr.intercept_ + 2, 50)\n",
    "beta_1 = np.linspace(regr.coef_ - 0.02, regr.coef_ + 0.02, 50)\n",
    "xx, yy = np.meshgrid(beta_0, beta_1, indexing=\"xy\")\n",
    "Z = np.zeros((beta_0.size, beta_1.size))\n",
    "\n",
    "# Calculate Z-values (RSS) based on grid of coefficients\n",
    "for (i, j), v in np.ndenumerate(Z):\n",
    "    Z[i, j] = ((y - (xx[i, j] + X.ravel() * yy[i, j])) ** 2).sum() / 1000\n",
    "\n",
    "# minimised RSS\n",
    "min_RSS = r\"$\\beta_0$, $\\beta_1$ for minimised RSS\"\n",
    "min_rss = (\n",
    "    np.sum((regr.intercept_ + regr.coef_ * X - y.values.reshape(-1, 1)) ** 2) / 1000\n",
    ")\n",
    "min_rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the RSS with corresponding $\\beta_0$ and $\\beta_1$ in 2D and 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "fig.suptitle(\"RSS - Regression coefficients\", fontsize=20)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "\n",
    "# Left plot\n",
    "CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3])\n",
    "ax1.scatter(regr.intercept_, regr.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax1.clabel(CS, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "\n",
    "# Right plot\n",
    "ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3)\n",
    "ax2.contour(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    zdir=\"z\",\n",
    "    offset=Z.min(),\n",
    "    cmap=plt.cm.Set1,\n",
    "    alpha=0.4,\n",
    "    levels=[2.15, 2.2, 2.3, 2.5, 3],\n",
    ")\n",
    "ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=\"r\", label=min_RSS)\n",
    "ax2.set_zlabel(\"RSS\")\n",
    "ax2.set_zlim(Z.min(), Z.max())\n",
    "ax2.set_ylim(0.02, 0.07)\n",
    "\n",
    "# settings common to both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=17)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=17)\n",
    "    ax.set_yticks([0.03, 0.04, 0.05, 0.06])\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example explanation of system transparency\n",
    "\n",
    "Run the cell below for understanding the system transparency for linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(\n",
    "    x=advertising_df.TV,\n",
    "    y=advertising_df.Sales,\n",
    "    order=1,\n",
    "    x_ci=\"ci\",\n",
    "    scatter_kws={\"color\": \"r\", \"s\": 9},\n",
    ")\n",
    "\n",
    "x1 = 100\n",
    "y1 = (regr.intercept_ + regr.coef_ * (x1 - scale.mean_))[0]\n",
    "print(\"Predicted sales for TV = 100: \", y1)\n",
    "plt.plot((x1, x1), (0, y1), \"k--\", lw=1)\n",
    "plt.plot((-10, x1), (y1, y1), \"k--\", lw=1)\n",
    "\n",
    "x_mean = scale.mean_[0]\n",
    "y_ = (regr.intercept_ + regr.coef_ * (x_mean - scale.mean_))[0]\n",
    "print(\"Predicted sales for TV = %s (mean): \" % x_mean, y_)\n",
    "plt.plot((x_mean, x_mean), (0, y_), \"k--\", lw=1)\n",
    "plt.plot((-10, x_mean), (y_, y_), \"k--\", lw=1)\n",
    "\n",
    "x2 = 200\n",
    "y2 = (regr.intercept_ + regr.coef_ * (x2 - scale.mean_))[0]\n",
    "print(\"Predicted sales for TV = 200: \", y2)\n",
    "plt.plot((x2, x2), (0, y2), \"k--\", lw=1)\n",
    "plt.plot((-10, x2), (y2, y2), \"k--\", lw=1)\n",
    "\n",
    "plt.xlim(-10, 310)\n",
    "plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we learnt a linear regression model $f(x)$ with two parameters, $\\beta_0$ and $\\beta_1$, from the data, where\n",
    "- $\\beta_0 = 14.0225 $ is the `sales` when input value `TV` equals to its mean, i.e., 147.0425, and\n",
    "- $\\beta_1 = 0.0475 $ is the change of units in the `sales` when `TV` increases by 1 unit.\n",
    "\n",
    "Using these two estimated parameters, we can examine the system logic of the simple linear regression model to reveal its system transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- When `TV`$=200$, the predicted `sales` $f(200) \\approx 16.54 = \\beta_1 \\times (200 - 147.04) + f(147.04)$, which can be derived from the linear regression equation: $f(x) = \\beta_1 \\times (x - \\bar{x}) + \\beta_0 = \\beta_1 \\times (x - \\bar{x}) + f(\\bar{x}) $. \n",
    "\n",
    "- To produce an estimated `sales` of $\\hat{y}$, e.g. 18, we locate 18 on the vertical axis and then move horizontally to find the fitted line to find the corresponding `TV` value on the horizontal axis, i.e., ~230, which can be analytically obtained using the inverse function of the learnt linear regression model $f(x)$ $x = \\frac{\\hat{y} - \\beta_0}{\\beta_1} + \\bar{x}$ (giving 230.78).\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing model accuracy via $R^2$\n",
    "\n",
    "The accuracy of the linear model is dependent on the variability of the response $y$ and the predictor $x$. The variability of $y$ is measured by the variance of $y$, denoted by $\\sigma_y^2$. The variability of $x$ is measured by the variance of $x$, denoted by $\\sigma_x^2$. The [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), denoted by $R^2$, is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{TSS} = \\sum_{i=1}^N (y_i - \\bar{y})^2$ *is the total sum of squares*. Dividing the RSS by the total number of training samples gives the *mean squared error* (MSE) of the model. The MSE is the average squared distance between the observed response values and the response values predicted by the model. The MSE is also known as the *mean squared prediction error* (MSPE). The MSE is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MSE} = \\frac{1}{N} \\text{RSS} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "The coefficient of determination $R^2$ measures the proportion of the _total_ variance in the response $y$ that is explained by the linear model using the predictor $x$. $R^2$ is always between 0 and 1: it is 0 when the regression line does not fit the data at all, and it is 1 when the regression line perfectly fits the data. $R^2$ is also known as the *coefficient of multiple determination* and it is a measure of the goodness of fit of the linear model. \n",
    "\n",
    "Watch the 11-minute video below to learn more about $R^2$\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/2AQKmw14mHM?start=16\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Explaining $R^2$, by StatQuest](https://www.youtube.com/embed/2AQKmw14mHM?start=16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to fit a simple linear regression model using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "X = advertising_df.TV.values.reshape(-1, 1)\n",
    "y = advertising_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(regr.intercept_)\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then evaluate the learnt model with $R^2$ via [`r2_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) (Table 3.1 & 3.2 of the text book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_pred = regr.predict(X)\n",
    "print(\"R2 score:\", r2_score(y, sales_pred))\n",
    "print(\"Mean squared error: \", mean_squared_error(y, sales_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can see that the simple linear regression model explains about 61% of the variance of the response data around its mean, leaving about 39% (RSS) unexplained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors and residual standard error\n",
    "\n",
    "*Advanced: advanced contents are optional.\n",
    "\n",
    "The linear relationship between $x$ and $y$ can be written in an equation, rather than an approximation earlier in Equation {eq}`y_approx_x`, as\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\epsilon$ is a random error term that represents the difference (i.e. the error, unexplained part) between the observed response $y$ and the true response $\\beta_0 + \\beta_1 x$. The error term $\\epsilon$ is assumed to be normally distributed with mean zero and constant variance $\\sigma^2$. The coefficient estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are only estimates of the true coefficients $\\beta_0$ and $\\beta_1$. We can quantify the accuracy of the estimates by computing the [*standard error*](https://en.wikipedia.org/wiki/Standard_error) of the estimates. The following formulars can be used to compute the standard error associated with $\\hat{\\beta}_1$ and $\\hat{\\beta}_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{SE}(\\hat{\\beta}_{1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^N (x_i - \\bar{x})^2}, \\quad \\text{SE}(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[\\frac{1}{N} + \\frac{\\bar{x}^2}{\\sum_{i=1}^N (x_i - \\bar{x})^2} \\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma^2$ is an estimate of the variance of the error term $\\epsilon= y - (\\beta_0 + \\beta_1 x) $, $\\hat{y}_i$ is the $i\\text{th}$ observation of the predicted response $\\hat{y}$, and $x_i$ is the $i\\text{th}$ observation of the predictor $x$, $\\bar{x}$ and $\\bar{y}$ are the sample means of $x$ and $y$ respectively. \n",
    "\n",
    "In general, $\\sigma^2$ is unknown, so we need to estimate it from the *residual standard error* (RSE) below\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSE} = \\sqrt{\\frac{1}{N-2}\\text{RSS}} = \\sqrt{\\frac{1}{N-2} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2}.\n",
    "\\end{equation}\n",
    "\n",
    "RSE is also _another way to assess the quality of a linear regression fit_ (in terms of error). Due to the presence of the error term $\\epsilon$, perfect prediction is not possible even if we know the true regression line (unless $\\epsilon$ is zero, rarely the case in practice). The RSE is an estimate of the standard deviation of $\\epsilon$. It is the average amount that the response will deviate from the true regression line, measuring the lack of fit of the model to the data. The smaller the RSE, the better the model fits the data. \n",
    "\n",
    "## Confidence intervals\n",
    "\n",
    "The standard error indicates the average amount that an estimate differs from its actual value. Thus, the standard error of $\\hat{\\beta}_1$ is a measure of the average amount that $\\hat{\\beta}_1$ will deviate from the its true value $\\beta_1$. The standard error of $\\hat{\\beta}_0$ is a measure of the average amount that $\\hat{\\beta}_0$ will deviate from the its true value $\\beta_0$.\n",
    "\n",
    "Standard errors can be used to compute [confidence intervals](https://en.wikipedia.org/wiki/Confidence_interval). A 95\\% is defined as a range of values such that with 95 \\% probability, the range (interval) will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence intervals for $\\beta_1$ and $\\beta_0$ approximately takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_1 \\pm 2 \\times \\text{SE}(\\hat{\\beta}_1),\\:\\: \\hat{\\beta}_0 \\pm 2 \\times \\text{SE}(\\hat{\\beta}_0).\n",
    "\\end{equation}\n",
    "\n",
    "The above can be interpreted as there is an approximately 95% chance that the interview $[\\hat{\\beta}_1 - 2 \\times \\text{SE}(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\times \\text{SE}(\\hat{\\beta}_1)] will contain the true value of $\\beta_1$, and the interval $[\\hat{\\beta}_0 - 2 \\times \\text{SE}(\\hat{\\beta}_0), \\hat{\\beta}_0 + 2 \\times \\text{SE}(\\hat{\\beta}_0)]$ will contain the true value of $\\beta_0$.\n",
    "\n",
    "Run the following code to compute the statistics, including the confidence intervals, of the learnt model using `statesmodels` (page 67 & Table 3.1 & 3.2 of the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ TV\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above did _NOT_ standardise the input by removing its mean so its intercept differs from the one in the previous section. The above includes some additional statistics. The last two columns are the 95% confidence intervals. The 5th column is the $p$-values of the coefficients, which are used to test the significance of the coefficients. The $p$-values are computed using the [$t$-test](https://en.wikipedia.org/wiki/Student%27s_t-test) for the [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis) that the coefficient is zero.  The 4th column is the $t$ statistic value. The $p$-values are used to determine whether the coefficient is statistically significant. The smaller the $p$-value, the more likely it is that the coefficient is statistically significant. The $p$-values are also used to determine whether the model is statistically significant. The model is statistically significant if the $p$-value is less than the significance level (e.g. 0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the RSS computed via `statsmodels` is the same as that obtained via `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS with regression coefficients\n",
    "(\n",
    "    (advertising_df.Sales - (est.params[0] + est.params[1] * advertising_df.TV)) ** 2\n",
    ").sum() / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. This question involves the use of simple linear regression on the **[Carseats](https://github.com/pykale/transparentML/blob/main/data/Carseats.csv)** dataset.\n",
    "\n",
    "**a**. Use the **LinearRegression()** function to perform a simple linear regression with **Sales** as the response and **Price** as the predictor. Find out the weight and bias of the regression model.  Don't forget to use **StandardScaler** to preprocess the data before fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "carseats_df = pd.read_csv(\n",
    "    \"https://github.com/pykale/transparentML/raw/main/data/Carseats.csv\"\n",
    ")\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "scale = StandardScaler(with_mean=True, with_std=False)\n",
    "X = scale.fit_transform(carseats_df.Price.values.reshape(-1, 1))\n",
    "y = carseats_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(\"Regression model intercept (bias): \", regr.intercept_)\n",
    "print(\"Regression model slop (weight): \", regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Plot the response and predictor. Use the **regplot()** function to display the least-squared regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's plot our predicted regression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.regplot(\n",
    "    x=carseats_df.Price,\n",
    "    y=carseats_df.Sales,\n",
    "    order=1,\n",
    "    x_ci=\"ci\",\n",
    "    scatter_kws={\"color\": \"r\", \"s\": 9},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. We learnt a linear regression model $f(x)$ with two parameters weight($\\beta_1$) and bias($\\beta_0$), where we already have the weight and bias of the model from (a). Using these two estimated parameters, we can examine the system logic of the simple linear regression model to reveal its system transparency. Using the below linear regression equation predict the sales when Price = 77 and 134. Visualize them on the regression plot.\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_1 \\times (x - \\bar{x}) + \\beta_0 = \\beta_1 \\times (x - \\bar{x}) + f(\\bar{x}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "sns.regplot(\n",
    "    x=carseats_df.Price,\n",
    "    y=carseats_df.Sales,\n",
    "    order=1,\n",
    "    x_ci=\"ci\",\n",
    "    scatter_kws={\"color\": \"r\", \"s\": 9},\n",
    ")\n",
    "\n",
    "x1 = 77\n",
    "y1 = (regr.intercept_ + regr.coef_ * (x1 - scale.mean_))[0]\n",
    "print(\"Predicted sales for Price = 77: \", y1)\n",
    "plt.plot((x1, x1), (0, y1), \"k--\", lw=1)\n",
    "plt.plot((-10, x1), (y1, y1), \"k--\", lw=1)\n",
    "\n",
    "x2 = 134\n",
    "y2 = (regr.intercept_ + regr.coef_ * (x2 - scale.mean_))[0]\n",
    "print(\"Predicted sales for Price = 134: \", y2)\n",
    "plt.plot((x2, x2), (0, y2), \"k--\", lw=1)\n",
    "plt.plot((-10, x2), (y2, y2), \"k--\", lw=1)\n",
    "plt.xlim(20, 200)\n",
    "plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d**. Create the grid coordinates for plotting and compute the minimum of RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "beta_0 = np.linspace(regr.intercept_ - 2, regr.intercept_ + 2, 50)\n",
    "beta_1 = np.linspace(regr.coef_ - 0.02, regr.coef_ + 0.02, 50)\n",
    "xx, yy = np.meshgrid(beta_0, beta_1, indexing=\"xy\")\n",
    "Z = np.zeros((beta_0.size, beta_1.size))\n",
    "\n",
    "# Calculate Z-values (RSS) based on grid of coefficients\n",
    "for (i, j), v in np.ndenumerate(Z):\n",
    "    Z[i, j] = ((y - (xx[i, j] + X.ravel() * yy[i, j])) ** 2).sum() / 1000\n",
    "\n",
    "# minimised RSS\n",
    "min_RSS = r\"$\\beta_0$, $\\beta_1$ for minimised RSS\"\n",
    "min_rss = (\n",
    "    np.sum((regr.intercept_ + regr.coef_ * X - y.values.reshape(-1, 1)) ** 2) / 1000\n",
    ")\n",
    "print(\"Minimum of RSS is %f\" % min_rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.** Plot the RSS with corresponding bias and weight in 2D and 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "fig.suptitle(\"RSS - Regression coefficients\", fontsize=20)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "\n",
    "# Left plot\n",
    "CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.60, 2.65, 2.75, 2.9, 3])\n",
    "ax1.scatter(regr.intercept_, regr.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax1.clabel(CS, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "\n",
    "# Right plot\n",
    "ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3)\n",
    "ax2.contour(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    zdir=\"z\",\n",
    "    offset=Z.min(),\n",
    "    cmap=plt.cm.Set1,\n",
    "    alpha=0.4,\n",
    "    levels=[2.60, 2.65, 2.75, 2.9, 3],\n",
    ")\n",
    "ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=\"r\", label=min_RSS)\n",
    "ax2.set_zlabel(\"RSS\")\n",
    "ax2.set_zlim(Z.min(), Z.max())\n",
    "\n",
    "\n",
    "# settings common to both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=17)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=17)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f**. $R^2$ score always varies from -1 to 1.\n",
    "\n",
    "        a. True\n",
    "        \n",
    "        b. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the solution below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**b. False**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g.** Evaluate the learnt model accuracy using $R^2$ score and Means squared error(MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = regr.predict(X)\n",
    "print(\"R2 score:\", r2_score(y, y_pred))\n",
    "print(\"Mean squared error: \", mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h**. Is the model fitted the data well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the solution below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**No. As we know $R^2$ is 0 when the model does not fit the data at all, and it is 1 when the model perfectly fits the data. We got $R^2$ score of 0.197 which pretty low and near to 0 which means the model does fit the data well.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i.** Use the **statsmodels** library to learnt a model and compute the statistics, including the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "est = ols(\"Sales ~ Price\", carseats_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I**. What is the minimum value of $95\\%$ confidence intervals in this model for weight parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the solution below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**12.398**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II**. Is there a relationship between the predictor and the response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the solution below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**Yes, the low P-value associated with the t-statistic for price suggests so.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III**. How strong is the relationship between the predictor and the response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the solution below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**For a unit increase in price, our model predicts sales will decrease by -0.0531. So for example, increasing price by 20 is expected to decrease efficiency by -1.062 sales.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV**.  Is the relationship between the predictor and the response positive or negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the solution below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**Negative**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**j.** Compute the RSS and Verify that the RSS computed via statsmodels is the same as that obtained via scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# RSS with regression coefficients\n",
    "(\n",
    "    (carseats_df.Sales - (est.params[0] + est.params[1] * carseats_df.Price)) ** 2\n",
    ").sum() / 1000"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
