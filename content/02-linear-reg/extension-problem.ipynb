{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions and problems\n",
    "\n",
    "This section introduces more complicated types of variables and relationships. We will use three datasets for illustration:\n",
    "1. The `Advertising` dataset that has been used in the previous sections. \n",
    "2. The `Credit` dataset contains information about credit card holders. The goal is to predict the credit `Limit` for each card holder. \n",
    "3. The `Auto` dataset contains information about the characteristics of cars. The goal is to predict the `mpg` (miles per gallon) for a car based on its features like `horsepower`, `weight` and `acceleration`. This dataset is also used in the  tutorial of {doc}`Prerequisites <../00-prereq/overview>`.\n",
    "\n",
    "### Import the required libraries and load datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load `Advertising`, `Credit`, and `Auto` datasets**. \n",
    "<!-- Datasets available on https://www.statlearning.com/resources-first-edition -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Advertising.csv\"\n",
    "advertising_df = pd.read_csv(data_url, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_url = \"https://github.com/pykale/transparentML/raw/main/data/Credit.csv\"\n",
    "\n",
    "credit_df = pd.read_csv(credit_url)\n",
    "credit_df[\"Student2\"] = credit_df.Student.map({\"No\": 0, \"Yes\": 1})\n",
    "credit_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_url = \"https://github.com/pykale/transparentML/raw/main/data/Auto.csv\"\n",
    "\n",
    "auto_df = pd.read_csv(auto_url, na_values=\"?\").dropna()\n",
    "auto_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative predictors\n",
    "\n",
    "In the previous sections, the predictor variables are *quantitative*. For example, in the `Credit` dataset, the response is `Balance` (average credit card debt for each individual) and there are six quantitative predictors: `Age` , `Cards` (number of credit cards), `Education` (years of education), `Income` (in thousands of dollars), `Limit` (credit limit), and `Rating` (credit rating). The following code displays the pairwise relationships between these variables (Figure 3.6 in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    credit_df[[\"Balance\", \"Age\", \"Cards\", \"Education\", \"Income\", \"Limit\", \"Rating\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in practice, predictor variables are not always quantitative. For example in `Credit` dataset, it also contains four qualitative variables: `Student` (`Yes` or `No`), `Own` (`Yes` or `No`), `Married` (`Yes` or `No`), and `Region` (`South`, `East`, or `West`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictors with only two levels**\n",
    "\n",
    "Run the following Least squares coefficient estimates associated with the regression of `Balance` onto `Own` in the `Credit` data set (Table 3.7 in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Balance ~ Own\", credit_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictors with more than two levels**\n",
    "\n",
    "Run the following Least squares coefficient estimates associated with the regression of `Balance` onto `Region` in the `Credit` data set (Table 3.8 in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Balance ~ Region\", credit_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions of linear regression\n",
    "\n",
    "#### Interaction between variables\n",
    "\n",
    "In the previous analysis of `Advertising` dataset, the predictor variables are assumed to be independent. However, this model may be incorrect. For example, `radio` advertising can increase the effectiveness of `TV` advertising, which is known as *interaction* effect in statistics. Consider the a standard multiple linear regression with two variables\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon.\n",
    "$$\n",
    "\n",
    "This model can be extended to include an interaction term between $x_1$ and $x_2$:\n",
    "\n",
    "\\begin{align}\n",
    "y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon \\\\\n",
    "&= \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon \\\\\n",
    "&= \\beta_0 + \\tilde{\\beta}_1 x_1 + \\beta_2 x_2 + \\epsilon,\n",
    "\\end{align}\n",
    "\n",
    "where $\\tilde{\\beta}_1 = \\beta_1 + \\beta_3 x_2$ is the *effective* coefficient on $x_1$. Then the association between $y$ and $x_1$ is no longer constant and depends on the value of $x_2$. Similarly, the association between $y$ and $x_2$ is no longer constant and depends on the value of $x_1$.\n",
    "\n",
    "Run the following code for the `Advertising` data, least squares coefficient estimates associated with the regression of sales onto `TV` and `Radio`, with an interaction term, `TV` $\\times$ `Radio` (Table 3.9 of the text book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ TV + Radio + TV*Radio\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction between qualitative and quantitative variables\n",
    "\n",
    "The concept of interactions applies just as well to qualitative variables. In fact, sometimes an interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation. For example, run the following code to predict `Balance` using the `Income` (quantitative) and `Student` (qualitative) variables, and compare the differences between including and excluding the interaction term between `Income` and `Student` (Figure 3.7 of the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1 = ols(\"Balance ~ Income + Student2\", credit_df).fit()\n",
    "est2 = ols(\"Balance ~ Income + Income*Student2\", credit_df).fit()\n",
    "\n",
    "print(\"Regression 1 - without interaction term\")\n",
    "print(est1.params)\n",
    "print(\"\\nRegression 2 - with interaction term\")\n",
    "print(est2.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income (x-axis)\n",
    "regr1 = est1.params\n",
    "regr2 = est2.params\n",
    "\n",
    "income = np.linspace(0, 150)\n",
    "\n",
    "# Balance without interaction term (y-axis)\n",
    "student1 = np.linspace(\n",
    "    regr1[\"Intercept\"] + regr1[\"Student2\"],\n",
    "    regr1[\"Intercept\"] + regr1[\"Student2\"] + 150 * regr1[\"Income\"],\n",
    ")\n",
    "non_student1 = np.linspace(\n",
    "    regr1[\"Intercept\"], regr1[\"Intercept\"] + 150 * regr1[\"Income\"]\n",
    ")\n",
    "\n",
    "# Balance with interaction term (y-axis)\n",
    "student2 = np.linspace(\n",
    "    regr2[\"Intercept\"] + regr2[\"Student2\"],\n",
    "    regr2[\"Intercept\"]\n",
    "    + regr2[\"Student2\"]\n",
    "    + 150 * (regr2[\"Income\"] + regr2[\"Income:Student2\"]),\n",
    ")\n",
    "non_student2 = np.linspace(\n",
    "    regr2[\"Intercept\"], regr2[\"Intercept\"] + 150 * regr2[\"Income\"]\n",
    ")\n",
    "\n",
    "# Create plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.plot(income, student1, \"r\", income, non_student1, \"k\")\n",
    "ax2.plot(income, student2, \"r\", income, non_student2, \"k\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.legend([\"student\", \"non-student\"], loc=2)\n",
    "    ax.set_xlabel(\"Income\")\n",
    "    ax.set_ylabel(\"Balance\")\n",
    "    ax.set_ylim(ymax=1550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear relationships\n",
    "\n",
    "In some cases, the true relationship between the response and the predictors may be non-linear. Here we present a very simple way to directly extend the linear model to accommodate non-linear relationships, using *polynomial regression*.\n",
    "\n",
    "Run the following code to display the relationship between `mpg` (gas mileage in miles per gallon) and different degrees of `horsepower` (Figure 3.8 of the textbook) for a number of cars in the `Auto` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Seaborn's regplot() you can easily plot higher order polynomials.\n",
    "plt.scatter(\n",
    "    auto_df.horsepower, auto_df.mpg, facecolors=\"None\", edgecolors=\"k\", alpha=0.5\n",
    ")\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Linear\",\n",
    "    scatter=False,\n",
    "    color=\"orange\",\n",
    ")\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Degree 2\",\n",
    "    order=2,\n",
    "    scatter=False,\n",
    "    color=\"lightblue\",\n",
    ")\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Degree 5\",\n",
    "    order=5,\n",
    "    scatter=False,\n",
    "    color=\"g\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.ylim(5, 55)\n",
    "plt.xlim(40, 240);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure seem to suggest that a quadratic relationship between `mpg` and `horsepower` might be more appropriate than a linear relationship. We can fit such a model using the following regression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{mpg} = \\beta_0 + \\beta_1 \\times \\textrm{horsepower} + \\beta_2 \\times \\textrm{horsepower}^2 + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "In fact, this is still a linear model with $x_1 = \\textrm{horsepower}$ and $x_2 = \\textrm{horsepower}^2$. Run the following code to fit a linear regression model with `mpg` as the response and `horsepower` and `horsepower^2` as the predictors (Table 3.10 of the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df[\"horsepower2\"] = auto_df.horsepower**2\n",
    "auto_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"mpg ~ horsepower + horsepower2\", auto_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems of linear regression\n",
    "\n",
    "#### Non-linearity of the Data\n",
    "  \n",
    "The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. In addition, the prediction accuracy of the model can be significantly reduced.\n",
    "\n",
    "Run the following code to compare the differences in accuracy between a linear regression model and a quadratic regression model for regressing `mpg` onto `horsepower`.\n",
    "\n",
    "Firstly, fitting a linear regression model and a quadratic regression model for regressing `mpg` onto `horsepower` and compute the residuals for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "# Linear fit\n",
    "X = auto_df.horsepower.values.reshape(-1, 1)\n",
    "y = auto_df.mpg\n",
    "regr.fit(X, y)\n",
    "\n",
    "auto_df[\"pred1\"] = regr.predict(X)\n",
    "auto_df[\"resid1\"] = auto_df.mpg - auto_df.pred1\n",
    "\n",
    "# Quadratic fit\n",
    "X2 = auto_df[[\"horsepower\", \"horsepower2\"]].values\n",
    "regr.fit(X2, y)\n",
    "\n",
    "auto_df[\"pred2\"] = regr.predict(X2)\n",
    "auto_df[\"resid2\"] = auto_df.mpg - auto_df.pred2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots of the residuals for each model. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left plot\n",
    "sns.regplot(\n",
    "    x=auto_df.pred1,\n",
    "    y=auto_df.resid1,\n",
    "    lowess=True,\n",
    "    ax=ax1,\n",
    "    line_kws={\"color\": \"r\", \"lw\": 1},\n",
    "    scatter_kws={\"facecolors\": \"None\", \"edgecolors\": \"k\", \"alpha\": 0.5},\n",
    ")\n",
    "ax1.hlines(\n",
    "    0,\n",
    "    xmin=ax1.xaxis.get_data_interval()[0],\n",
    "    xmax=ax1.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dotted\",\n",
    ")\n",
    "ax1.set_title(\"Residual Plot for Linear Fit\")\n",
    "\n",
    "# Right plot\n",
    "sns.regplot(\n",
    "    x=auto_df.pred2,\n",
    "    y=auto_df.resid2,\n",
    "    lowess=True,\n",
    "    line_kws={\"color\": \"r\", \"lw\": 1},\n",
    "    ax=ax2,\n",
    "    scatter_kws={\"facecolors\": \"None\", \"edgecolors\": \"k\", \"alpha\": 0.5},\n",
    ")\n",
    "ax2.hlines(\n",
    "    0,\n",
    "    xmin=ax2.xaxis.get_data_interval()[0],\n",
    "    xmax=ax2.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dotted\",\n",
    ")\n",
    "ax2.set_title(\"Residual Plot for Quadratic Fit\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Fitted values\")\n",
    "    ax.set_ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{Admonition} How to interpret the plots?\n",
    ":class: tip, dropdown\n",
    "- The left panel of displays a residual plot from the linear regression of `mpg` onto `horsepower` on the `Auto` dataset. The red line is a smooth fit to the residuals, which is displayed in\n",
    "order to make it easier to identify any trends. The residuals exhibit a clear U-shape, which provides a strong indication of non-linearity in the data.\n",
    "- The right-hand panel displays the residual plot that results from the model contains a quadratic term. There appears to be little pattern in the residuals, suggesting that the quadratic term improves the fit to the data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collinearity\n",
    "  \n",
    "Collinearity refers to the situation in which two or more predictor variables are closely related to one another. \n",
    "  \n",
    " Run the following code to illustrate the problem of collinearity in the `Credit` dataset (Figure 3.14 of the textbook). In the left-hand panel, the two predictors `limit` and `age` appear to have no obvious relationship. In contrast, in the right-hand panel, the predictors `limit` and `rating` are very highly correlated with each other, and we say that they are *collinear*. In this context, `limit` and `rating` tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response, `balance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left plot\n",
    "ax1.scatter(credit_df.Limit, credit_df.Age, facecolor=\"None\", edgecolor=\"r\")\n",
    "ax1.set_ylabel(\"Age\")\n",
    "\n",
    "# Right plot\n",
    "ax2.scatter(credit_df.Limit, credit_df.Rating, facecolor=\"None\", edgecolor=\"r\")\n",
    "ax2.set_ylabel(\"Rating\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Limit\")\n",
    "    ax.set_xticks([2000, 4000, 6000, 8000, 12000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run following code to learn the difficulties that can result from collinearity in the context of the `Credit` dataset (Table 3.11 of the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = credit_df.Balance\n",
    "\n",
    "# Regression for left plot\n",
    "X = credit_df[[\"Age\", \"Limit\"]].values\n",
    "regr1 = LinearRegression()\n",
    "regr1.fit(scale(X.astype(\"float\"), with_std=False), y)\n",
    "print(\"Age/Limit\\n\", regr1.intercept_)\n",
    "print(regr1.coef_)\n",
    "\n",
    "# Regression for right plot\n",
    "X2 = credit_df[[\"Rating\", \"Limit\"]].values\n",
    "regr2 = LinearRegression()\n",
    "regr2.fit(scale(X2.astype(\"float\"), with_std=False), y)\n",
    "print(\"\\nRating/Limit\\n\", regr2.intercept_)\n",
    "print(regr2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create grid coordinates for plotting and then calculate RSS based on grid of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid coordinates\n",
    "beta_age = np.linspace(regr1.coef_[0] - 3, regr1.coef_[0] + 3, 100)\n",
    "beta_limit = np.linspace(regr1.coef_[1] - 0.02, regr1.coef_[1] + 0.02, 100)\n",
    "\n",
    "beta_rating = np.linspace(regr2.coef_[0] - 3, regr2.coef_[0] + 3, 100)\n",
    "beta_limit2 = np.linspace(regr2.coef_[1] - 0.2, regr2.coef_[1] + 0.2, 100)\n",
    "\n",
    "X1, Y1 = np.meshgrid(beta_limit, beta_age, indexing=\"xy\")\n",
    "X2, Y2 = np.meshgrid(beta_limit2, beta_rating, indexing=\"xy\")\n",
    "Z1 = np.zeros((beta_age.size, beta_limit.size))\n",
    "Z2 = np.zeros((beta_rating.size, beta_limit2.size))\n",
    "\n",
    "limit_scaled = scale(credit_df.Limit.astype(\"float\"), with_std=False)\n",
    "age_scaled = scale(credit_df.Age.astype(\"float\"), with_std=False)\n",
    "rating_scaled = scale(credit_df.Rating.astype(\"float\"), with_std=False)\n",
    "\n",
    "# calculate RSS\n",
    "for (i, j), v in np.ndenumerate(Z1):\n",
    "    Z1[i, j] = (\n",
    "        (y - (regr1.intercept_ + X1[i, j] * limit_scaled + Y1[i, j] * age_scaled)) ** 2\n",
    "    ).sum() / 1000000\n",
    "\n",
    "for (i, j), v in np.ndenumerate(Z2):\n",
    "    Z2[i, j] = (\n",
    "        (y - (regr2.intercept_ + X2[i, j] * limit_scaled + Y2[i, j] * rating_scaled))\n",
    "        ** 2\n",
    "    ).sum() / 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the contours of the RSS with respect to the coefficients of the two linear regression models in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import contour\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.suptitle(\"RSS - Regression coefficients\", fontsize=20)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "min_RSS = r\"$\\beta_0$, $\\beta_1$ for minimized RSS\"\n",
    "\n",
    "# Left plot\n",
    "contour1 = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8])\n",
    "ax1.scatter(regr1.coef_[1], regr1.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax1.clabel(contour1, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "ax1.set_ylabel(r\"$\\beta_{Age}$\", fontsize=17)\n",
    "\n",
    "# Right plot\n",
    "contour2 = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8])\n",
    "ax2.scatter(regr2.coef_[1], regr2.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax2.clabel(contour2, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "ax2.set_ylabel(r\"$\\beta_{Rating}$\", fontsize=17)\n",
    "ax2.set_xticks([-0.1, 0, 0.1, 0.2])\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r\"$\\beta_{Limit}$\", fontsize=17)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left: A contour plot of RSS for the regression of `balance` onto `age` and `limit`. The minimum value is well defined. Right: A contour plot of RSS for the regression of `balance` onto `rating` and `limit`. Because of the collinearity, there are many pairs ($\\beta_{\\text{Limit}}$ , $\\beta_{\\text{Rating}}$) with a similar value for RSS.\n",
    "\n",
    "Collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\\hat{\\beta}_j$ to grow. in the presence of collinearity, we may fail to reject $H_0$ : $\\hat{\\beta}_j = 0$. This means that the power of the hypothesis test—the probability of correctly power detecting a non-zero coefficient is reduced by collinearity.\n",
    "\n",
    "Run the following code to compare the coefficient estimates obtained from two separate multiple regression models, where the first model is a regression of `balance` on `age` and `limit`, and the second model is a regression of `balance` on `rating` and `limit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_age_limit = ols(\"Balance ~ Age + Limit\", credit_df).fit()\n",
    "est_age_limit.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_rating_limit = ols(\"Balance ~ Rating + Limit\", credit_df).fit()\n",
    "\n",
    "# est_limit = ols(\"Limit ~ Age + Rating\", credit_df).fit()\n",
    "\n",
    "# print(1 / (1 - est_rating.rsquared))\n",
    "# print(1 / (1 - est_age.rsquared))\n",
    "# print(1 / (1 - est_limit.rsquared))\n",
    "\n",
    "est_rating_limit.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} How to interpret the results? \n",
    ":class: tip, dropdown\n",
    "In the first regression, both `age` and `limit` are highly significant with very small p-values. In the second, the collinearity between `limit` and `rating` has caused the standard error for the `limit` coefficient estimate to increase by a factor of 12 and the p-value to increase to 0.701. In other words, the importance of the `limit` variable has been masked due to the presence of collinearity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. This question involves the use of simple linear regression on the **Auto** dataset from [Tabel 1.2](https://pykale.github.io/transparentML/01-intro/organisation.html#datasets-table).\n",
    "\n",
    "**a**. Use the **LinearRegression()** function to perform a simple linear regression with **mpg** as the response and **horsepower** as the predictor. Use the **summary()** function to print the results. Comment on the output.\n",
    "\n",
    "   For example:\n",
    "\n",
    "   >i. Is there a relationship between the predictor and the response?\n",
    "\n",
    "   >ii. How strong is the relationship between the predictor and the response?\n",
    "\n",
    "   >iii. Is the relationship between the predictor and the response positive or negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "auto_df = pd.read_csv(\"https://github.com/pykale/transparentML/raw/main/data/Auto.csv\")\n",
    "\n",
    "# Remove missing values\n",
    "auto_df = auto_df.drop(auto_df[auto_df.values == \"?\"].index)\n",
    "auto_df = auto_df.reset_index()\n",
    "\n",
    "\n",
    "# Convert quantitive datatypes to numerics\n",
    "datatypes = {\n",
    "    \"quant\": [\n",
    "        \"mpg\",\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "        \"weight\",\n",
    "        \"acceleration\",\n",
    "        \"year\",\n",
    "    ],\n",
    "    \"qual\": [\"origin\", \"name\"],\n",
    "}\n",
    "\n",
    "quants = auto_df[datatypes[\"quant\"]].astype(np.float_)\n",
    "\n",
    "auto_df = pd.concat([quants, auto_df[datatypes[\"qual\"]]], axis=1)\n",
    "\n",
    "# The statsmodels library provides a convenient means to get the\n",
    "# same statistics\n",
    "\n",
    "X = auto_df[\"horsepower\"]\n",
    "X = sm.add_constant(X)  # add bias constant\n",
    "y = auto_df[\"mpg\"]\n",
    "\n",
    "results = sm.OLS(y, X).fit()\n",
    "print(results.summary())\n",
    "\n",
    "\n",
    "# i. Yes, the low P-value associated with the t-statistic for horsepower suggests so.\n",
    "# ii. For a unit increase in horsepower, our model predicts mpg will decrease by -0.1578. So for example, increasing horsepower by 10 is expected to decrease efficiency by -1.578 mpg.\n",
    "# iii. Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Plot the response and predictor. Use the **regplot()** function to display the least-squared regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's plot our predicted regression\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = results.predict(X)\n",
    "df = pd.concat([auto_df[\"horsepower\"], auto_df[\"mpg\"]], axis=1)\n",
    "ax = sns.scatterplot(x=\"horsepower\", y=\"mpg\", data=df)\n",
    "ax.plot(auto_df[\"horsepower\"], y_pred, color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Produce the diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Providing powerful residual plots for simple AND multivariate\n",
    "# linear regresssion\n",
    "# - bring your own predictions\n",
    "# - underlying stats available as pandas dataframe\n",
    "# - visualise linearity and outliers in multiple dimensions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def lm_stats(X, y, y_pred):\n",
    "    \"\"\"LEVERAGE & STUDENTISED RESIDUALS\n",
    "    - https://en.wikipedia.org/wiki/Studentized_residual#How_to_studentize\n",
    "    \"\"\"\n",
    "    # Responses as np array vector\n",
    "    try:\n",
    "        y.shape[1] == 1\n",
    "        # take first dimension as vector\n",
    "        y = y.iloc[:, 0]\n",
    "    except:\n",
    "        pass\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Residuals\n",
    "    residuals = np.array(y - y_pred)\n",
    "\n",
    "    # Hat matrix\n",
    "    H = np.array(X @ np.linalg.inv(X.T @ X)) @ X.T\n",
    "\n",
    "    # Leverage\n",
    "    h_ii = H.diagonal()\n",
    "\n",
    "    ## Externally studentised residual\n",
    "    # In this case external studentisation is most appropriate\n",
    "    # because we are looking for outliers.\n",
    "\n",
    "    # Estimate variance (externalised)\n",
    "    σi_est = []\n",
    "    for i in range(X.shape[0]):\n",
    "        # exclude ith observation from estimation of variance\n",
    "        external_residuals = np.delete(residuals, i)\n",
    "        σi_est += [\n",
    "            np.sqrt(\n",
    "                (1 / (X.shape[0] - X.shape[1] - 1))\n",
    "                * np.sum(np.square(external_residuals))\n",
    "            )\n",
    "        ]\n",
    "    σi_est = np.array(σi_est)\n",
    "\n",
    "    # Externally studentised residuals\n",
    "    t = residuals / σi_est * np.sqrt(1 - h_ii)\n",
    "\n",
    "    # Return dataframe\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"residual\": residuals,\n",
    "            \"leverage\": h_ii,\n",
    "            \"studentised_residual\": t,\n",
    "            \"y_pred\": y_pred,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def lm_plot(lm_stats_df):\n",
    "    \"\"\"Provides residual plots based on results from lm_stat()\"\"\"\n",
    "    # Parse stats\n",
    "    t = lm_stats_df[\"studentised_residual\"]\n",
    "    h_ii = lm_stats_df[\"leverage\"]\n",
    "    y_pred = lm_stats_df[\"y_pred\"]\n",
    "\n",
    "    # setup axis for grid\n",
    "    plt.figure(1, figsize=(16, 18))\n",
    "\n",
    "    # Studentised residual plot\n",
    "    plt.subplot(321)\n",
    "    ax = sns.regplot(x=y_pred, y=t, lowess=True)\n",
    "    plt.xlabel(\"Fitted values\")\n",
    "    plt.ylabel(\"Studentised residuals\")\n",
    "    plt.title(\"Externally studentised residual plot\", fontweight=\"bold\")\n",
    "    # Draw Hastie and Tibshirani's bounds for possible outliers\n",
    "    ax.axhline(y=3, color=\"r\", linestyle=\"dashed\")\n",
    "    ax.axhline(y=-3, color=\"r\", linestyle=\"dashed\")\n",
    "\n",
    "    # Normal Q-Q plot\n",
    "    plt.subplot(322)\n",
    "    ax = stats.probplot(t, dist=\"norm\", plot=plt)\n",
    "    plt.ylabel(\"Studentised residuals\")\n",
    "    plt.title(\"Normal Q-Q\", fontweight=\"bold\")\n",
    "\n",
    "    # Standardised residuals\n",
    "    plt.subplot(323)\n",
    "    ax = sns.regplot(x=y_pred, y=np.sqrt(np.abs(t)), lowess=True)\n",
    "    plt.xlabel(\"Fitted values\")\n",
    "    plt.ylabel(\"√Standardized residuals\")\n",
    "    plt.title(\"Scale-Location\", fontweight=\"bold\")\n",
    "\n",
    "    # Residuals vs Leverage plot\n",
    "    plt.subplot(324)\n",
    "    ax = sns.scatterplot(x=h_ii, y=t)\n",
    "    plt.xlabel(\"Leverage\")\n",
    "    plt.ylabel(\"Studentised residuals\")\n",
    "    plt.title(\"Externally studentised residual vs Leverage\", fontweight=\"bold\")\n",
    "\n",
    "\n",
    "X = pd.concat([auto_df[\"horsepower\"]], axis=1)\n",
    "# Create the Design Matrix by adding constant bias variable\n",
    "intercept_const = pd.DataFrame({\"intercept\": np.ones(X.shape[0])})\n",
    "X = np.array(pd.concat([intercept_const, X], axis=1))\n",
    "\n",
    "y = auto_df[\"mpg\"]\n",
    "\n",
    "lm_plot(lm_stats(X, y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "```{toggle}\n",
    "\n",
    "The above residual plot grid shows the relationship between the horsepower predictor and the mpg response. There are several things to note:\n",
    "\n",
    "* Non-linearity of the data: The top-left residual plot exhibits a discernable pattern, in this case u-shaped, that suggests our linear model is not providing a optimal fit to our data - the relationship is non-linear. A discernable pattern in this plot suggests that our model is failing to account for some of the reducible variance in the responses. There is still a discernable pattern in the bottom-left plot suggesting that a quadratic transform only improves the fit of our model slightly.\n",
    "\n",
    "* Heteroscedasticity – Non-constant variance of error terms The top-left residual plot exhibits a conical shape. This suggests that there is some heteroscedasticity in our predictor. The standardised plot (bottom-left) also exhibits this characteristic suggesting that standardisation doesn't alleviate the issue – to address this we might consider fitting our model by weighted least squares.\n",
    "\n",
    "* Outliers and leverage: the bottom-right residual vs leverage plot suggests that there are several potential outliers (points in top-right of axis) that could be having a strong effect (leverage) on our model. We should add more predictors to our model to clarify outliers.\n",
    "\n",
    "* The top-right plot shows that our studentised residuals have a slightly non-normal distribution (TODO: ellaborate)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def lm_residual_corr_plot(lm_stats_df):\n",
    "    r = lm_stats_df[\"residual\"]\n",
    "    # Residuals correlation\n",
    "    plt.figure(1, figsize=(16, 5))\n",
    "    ax = sns.lineplot(x=list(range(r.shape[0])), y=r)\n",
    "    plt.xlabel(\"Observation\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.title(\"Correlation of error terms\", fontweight=\"bold\")\n",
    "\n",
    "\n",
    "lm_residual_corr_plot(lm_stats(X, y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "```{toggle}\n",
    "**Correlation of error terms:** The Correlation of Error Terms plot shows errors against ordered observations in our dataset. We see a slight increase in error above the 300th observation suggesting some correlation effect. This could mean that our estimated standard errors underestimate the true standard errors. Our confidence and prediction intervals may be narrower than they should be.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. This question involves the use of multiple linear regression on the **Carseats** dataset from Tabel 1.2.\n",
    "\n",
    "   **a**. Fit a multiple regression model to predict **Sales** using **Price**, **Urban**, and **US**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Provide an interpretation of each coefficient in the model. Be careful--some of the variables in the model are qualitative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Write out the model in equation form, being careful to handle the qualitative variables properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d**. For which of the predictors can you reject the null hypothesis $H_0 : \\beta_j = 0$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e**. On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f**. How well do the models in (a) and (e) fit the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g**. Using the model from (e), obtain 95% confidence intervals for the coefficient(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h**. Is there evidence of outliers or high-leverage observations in the model from (e)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. This question involves the **Boston** dataset from [Tabel 1.2](https://pykale.github.io/transparentML/01-intro/organisation.html#datasets-table). We will now try to predict per capita crime rate using the other variables in this dataset. In other owrds, per capita crime rate is the response, and the other variables are the prediction.\n",
    "\n",
    "   **a**. For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Fit a multiple regression model to predict the response using all of the predictors. Describe your results.. FOr which predictors can we reject the null hypothesis $H_0 : \\beta_j = 0$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. How do your results from (a) compare to your results form (b)? Create a plot displaying the uni-variate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d**. Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form,\n",
    "\n",
    "    $$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your answer with the reference solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
