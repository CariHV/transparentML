{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors classification\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a supervised learning algorithm. It is a _non-parametric_ method used for classification and regression. In both cases, the input consists of the $k$ closest training examples in the feature space. The output depends on whether KNN is used for classification or regression. Here, we focus on the classification case.\n",
    "\n",
    "### Ingredients & transparency\n",
    "For all machine learning models covered in this course, we aim to talk about their ingredients and transparency in a standard way to facilitate understanding their similarities and differences. For transparency, we will focus on the system transparency, i.e. system logic. Process transparency is not specific to ML models and it will be discussed when we cover the ML process/lifecycle.\n",
    "\n",
    "The ingredients of a KNN model are the training data. The transparency of a KNN model is the distance between the test point and the training points.\n",
    "\n",
    "```{admonition} Ingredients\n",
    "- Input: features of data samples\n",
    "- Output: class labels of data samples\n",
    "- Model: assigned dominant class label of the $K$ nearest neighbors of an input sample to it\n",
    "- Hyperparameter(s): the number of nearest neighbors $K$\n",
    "- Loss function: minimise distance between samples\n",
    "- Learning algorithm: sorting distances between samples\n",
    "```\n",
    "\n",
    "```{admonition} Transparency\n",
    "System logic\n",
    "- Condition to produce certain output: nearest neighbors for those samples with that label\n",
    "```\n",
    "\n",
    "### Example: Iris classification\n",
    "\n",
    "We adapt the [KNN example from scikit-learn](https://scikit-learn.org/1.0/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py) to illustrate the use of KNN for classification. \n",
    "\n",
    "To do so, we use the Iris dataset, which is a classic dataset in machine learning and statistics. It is included in scikit-learn and we load it as follows.\n",
    "\n",
    "```{admonition} Launch \n",
    "Click the rocket symbol (<i class=\"fas fa-rocket\"></i>) to launch this page as an interactive notebook in Google Colab (faster but requiring a Google account) or Binder.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Libraries\n",
    "\n",
    "Get ready by importing the application programming interfaces (APIs) needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data\n",
    "\n",
    " Let us work on one of the classical dataset in machine learning: the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). It can be loaded directly from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set hyperparameter\n",
    "\n",
    "Here, we set the only one hyperparameter, the number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visulisation in 2D\n",
    "\n",
    "For the purpose of visulisation to get intuition, we will work with the first two features only so that we can do visualisation of the data samples in a 2D plot. We can take the first two features by [indexing and slicing](https://pykale.github.io/transparentML/00-prereq/basic-python.html#indexing-and-slicing) as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n",
    "\n",
    "for weights in [\"uniform\", \"distance\"]:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    sns.scatterplot(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        hue=iris.target_names[y],\n",
    "        palette=cmap_bold,\n",
    "        alpha=1.0,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\n",
    "        \"3-Class classification (k = %i, weights = '%s')\" % (n_neighbors, weights)\n",
    "    )\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "To be completed in the next cycle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. How many features are there in total for the iris dataset? Write code below to find out or verify."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "1bda8b49161908208a6e3087b59be7e64a5f9b1866581a6eaef662e2ef65b61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
