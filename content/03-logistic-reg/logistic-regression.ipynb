{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic regression is not a regression, but a classification algorithm. It models the probabilities for classification problems.\n",
    "\n",
    "## Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import neighbors\n",
    "\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Default.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Note: factorize() returns two objects: a label array and an array with the unique values.\n",
    "# We are only interested in the first object.\n",
    "df[\"default2\"] = df.default.factorize()[0]\n",
    "df[\"student2\"] = df.student.factorize()[0]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a solution for classification. It models the probability that `y` belongs to a particular category rather than modelling this response `y` directly. Specifically, instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. \n",
    "\n",
    "$$\n",
    "\\textrm{logistic}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The step from linear regression to logistic regression is kind of straightforward. In the simple linear regression model, we have modelled the relationship between outcome and features with a linear equation:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1.\n",
    "$$\n",
    "\n",
    "For classification, we prefer probabilities between 0 and 1, so we wrap the right side of the equation into the logistic function. This forces the output to assume only values between 0 and 1.\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1)}}\n",
    "$$\n",
    "\n",
    "Run the code example has been used in the previous section again to see the curve of the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.balance.values.reshape(-1, 1)\n",
    "y = df.default2\n",
    "\n",
    "# Create array of test data. Calculate the classification probability\n",
    "# and predicted classification.\n",
    "X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1, 1)\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "clf.fit(X_train, y)\n",
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Left plot\n",
    "sns.regplot(\n",
    "    x=df.balance,\n",
    "    y=df.default2,\n",
    "    order=1,\n",
    "    ci=None,\n",
    "    scatter_kws={\"color\": \"orange\"},\n",
    "    line_kws={\"color\": \"lightblue\", \"lw\": 2},\n",
    "    ax=ax1,\n",
    ")\n",
    "# Right plot\n",
    "ax2.scatter(X_train, y, color=\"orange\")\n",
    "ax2.plot(X_test, prob[:, 1], color=\"lightblue\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.hlines(\n",
    "        1,\n",
    "        xmin=ax.xaxis.get_data_interval()[0],\n",
    "        xmax=ax.xaxis.get_data_interval()[1],\n",
    "        linestyles=\"dashed\",\n",
    "        lw=1,\n",
    "    )\n",
    "    ax.hlines(\n",
    "        0,\n",
    "        xmin=ax.xaxis.get_data_interval()[0],\n",
    "        xmax=ax.xaxis.get_data_interval()[1],\n",
    "        linestyles=\"dashed\",\n",
    "        lw=1,\n",
    "    )\n",
    "    ax.set_ylabel(\"Probability of default\")\n",
    "    ax.set_xlabel(\"Balance\")\n",
    "    ax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "    ax.set_xlim(xmin=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the coefficients and making predictions\n",
    "\n",
    "The coefficients of a logistic regression model can be estimated by maximum likelihood estimation. The likelihood function is\n",
    "\n",
    "$$\n",
    "L(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} \\mathbb{P}(y_i = 1) \\prod_{i:y_i=0} (1 - \\mathbb{P}(y_i = 1)).\n",
    "$$\n",
    "\n",
    "The mathematical details are beyond the scope of this course. Please read Section 4.3 of the [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/) book for more details of the optimization for logistic regression.\n",
    "\n",
    "To make predictions, taking the classification problem of the `Default` data for example, the probability of default given balance predicted by a logistic regression model is\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{default} = \\text{Yes} \\mid \\text{balance}, \\beta_0, \\beta_1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\text{balance})}}.\n",
    "$$\n",
    "\n",
    "In practice, we can use the `scikit-learn` or `statsmodels` package to fit a logistic regression model and making predictions. Run the following code to fit a logistic regression model using the `scikit-learn` and `statsmodels` packages, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "X_train = df.balance.values.reshape(-1, 1)\n",
    "y = df.default2\n",
    "clf.fit(X_train, y)\n",
    "print(clf)\n",
    "print(\"classes: \", clf.classes_)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ balance\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ student\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
