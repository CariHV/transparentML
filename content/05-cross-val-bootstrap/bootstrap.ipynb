{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap\n",
    "\n",
    "From the previous section, we have seen that it is common to encounter randomness in performance evaluation. Even with cross-validation, such randomness can still be present. In this case, it will be good to have an estimation of the uncertainty of the performance metric obtained. This is where [the bootstrap, or bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) comes in.\n",
    "\n",
    "The bootstrap is a widely applicable and extremely powerful resampling method that can be used to quantify the uncertainty associated with a performance metric for machine learning models. It is a non-parametric method, meaning that it does not make any assumptions about the underlying distribution of the data. It is also a model-free method, meaning that it does not require a model to be fit to the data.\n",
    "\n",
    "The bootstrap is a general technique for estimating the sampling distribution of a statistic by sampling from the data _with replacement_. The bootstrap can be used to estimate the sampling distribution of _ANY_ statistic, including the mean, standard deviation, correlation coefficient, and regression coefficients. The bootstrap can also be used to estimate the sampling distribution of a machine learning model, such as the coefficients of a linear regression model.\n",
    "  \n",
    "As a simple example, the bootstrap can be used to estimate the [standard errors](https://pykale.github.io/transparentML/02-linear-reg/simple-linear-regression.html#standard-errors-and-residual-standard-error) of the coefficients from a linear regression fit. Although standard errors were obtained automatically by `statsmodels` in {doc}`Linear regression <../00-prereq/overview>` and {doc}`Logistic regression <../03-logistic-reg/overview>`, the power of the bootstrap lies in the fact that it can be easily applied to a wide range of machine learning models.\n",
    "\n",
    "Watch the 6-minute video below for a visual explanation of cross-validation:\n",
    "\n",
    "```{admonition} Video\n",
    "\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/Xz0x-8-cgaQ?start=8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining Cross Bootstrap, by StatQuest](https://www.youtube.com/embed/Xz0x-8-cgaQ?start=8)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and load data\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    LeaveOneOut,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same [Auto dataset](https://github.com/pykale/transparentML/blob/main/data/Auto.csv) as in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_url = \"https://github.com/pykale/transparentML/raw/main/data/Auto.csv\"\n",
    "\n",
    "auto_df = pd.read_csv(auto_url, na_values=\"?\").dropna()\n",
    "auto_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap for uncertainty estimation\n",
    "\n",
    "Suppose that we wish to estimate the uncertainty of a coefficient estimate $\\beta_1$ from a linear regression fit, we take $M$ ($ M <N $) repeated samples with replacement from our dataset and train our linear regression model $B$ times and record each value $\\hat{\\beta}_1^{*1}, \\hat{\\beta}_1^{*2}, \\dots, \\hat{\\beta}_1^{*B}$. With enough resampling, e.g. $B = 1000$ or $B = 10,000$, we can plot the distribution of these bootstrapped estimates $\\hat{\\beta}_1^{*b}, b = 1,\\cdots, B $. Then, we can use the resulting distribution to quantify the variability (or uncertainty, confidence) of this estimate by calculating useful summary statistics, such as standard errors and confidence intervals.\n",
    "\n",
    "The power of the bootstrap lies in the ability to take repeated samples of the dataset, instead of collecting a new dataset each time. Also, in contrast to standard error estimates typically reported with statistical software that rely on algebraic methods and underlying assumptions (which may be wrong), bootstrapped standard error estimates are more accurate as they are calculated computationally without any pre-specified assumptions.\n",
    "\n",
    "<!-- **Bootstrap example using `scikit-learn`** (adapted from this [blog post](https://ethanwicker.com/2021-02-23-bootstrap-resampling-001/)) -->\n",
    "\n",
    "Let us study the bootstrap on the `Auto` dataset using the `scikit-learn's [`resample` API](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html) (adapted from this [blog post](https://ethanwicker.com/2021-02-23-bootstrap-resampling-001/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of iterations for bootstrap resample\n",
    "n_iterations = 1000\n",
    "\n",
    "# Initializing estimator\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Initializing DataFrame, to hold bootstrapped statistics\n",
    "bootstrapped_stats = pd.DataFrame()\n",
    "\n",
    "# Each loop iteration is a single bootstrap resample and model fit\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    # Sampling n_samples from data, with replacement, as train\n",
    "    # Defining test to be all observations not in train\n",
    "    train = resample(auto_df, replace=True, n_samples=len(auto_df))\n",
    "    test = auto_df[~auto_df.index.isin(train.index)]\n",
    "\n",
    "    X_train = train.loc[:, [\"horsepower\", \"weight\"]]\n",
    "    y_train = train.loc[:, [\"mpg\"]]\n",
    "\n",
    "    X_test = test.loc[:, [\"horsepower\", \"weight\"]]\n",
    "    y_test = test.loc[:, [\"mpg\"]]\n",
    "\n",
    "    # Fitting linear regression model\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Storing stats in DataFrame, and concatenating with stats\n",
    "    intercept = lin_reg.intercept_\n",
    "    beta_horsepower = lin_reg.coef_.ravel()[0]\n",
    "    beta_weight = lin_reg.coef_.ravel()[1]\n",
    "    r_squared = lin_reg.score(X_test, y_test)\n",
    "\n",
    "    bootstrapped_stats_i = pd.DataFrame(\n",
    "        data=dict(\n",
    "            intercept=intercept,\n",
    "            beta_horsepower=beta_horsepower,\n",
    "            beta_weight=beta_weight,\n",
    "            r_squared=r_squared,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    bootstrapped_stats = pd.concat(objs=[bootstrapped_stats, bootstrapped_stats_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect a few estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of the bootstrapped estimates of the coefficients and the corresponding test scores for the `Auto` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "sns.histplot(bootstrapped_stats[\"intercept\"], color=\"royalblue\", ax=axes[0], kde=True)\n",
    "sns.histplot(bootstrapped_stats[\"beta_horsepower\"], color=\"olive\", ax=axes[1], kde=True)\n",
    "sns.histplot(bootstrapped_stats[\"beta_weight\"], color=\"gold\", ax=axes[2], kde=True)\n",
    "sns.histplot(bootstrapped_stats[\"r_squared\"], color=\"teal\", ax=axes[3], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can easily estimate the uncertainty of ANY coefficient estimates and ANY test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "min 3 max 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
