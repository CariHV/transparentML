{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines\n",
    "\n",
    "In this section, we discuss the support vector machine (SVM), an approach for classification that was developed in the computer science community in the 1990s and that has grown in popularity since then. SVMs have been shown to perform well in a variety of settings, and are often considered one of the best \"out of the box\" classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Lab: 9.6.1 Support Vector Classifier](#9.6.1-Support-Vector-Classifier)\n",
    "- [Lab: 9.6.2 Support Vector Machine](#9.6.2-Support-Vector-Machine)\n",
    "- [Lab: 9.6.3 ROC Curves](#9.6.3-ROC-Curves)\n",
    "- [Lab: 9.6.4 SVM with Multiple Classes](#9.6.4-SVM-with-Multiple-Classes)\n",
    "- [Lab: 9.6.5 Application to Gene Expression Data](#9.6.5-Application-to-Gene-Expression-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum margin classifier\n",
    "\n",
    "In this section, we define a hyperplane and introduce the concept of an optimal separating hyperplane.\n",
    "\n",
    "In a $D$-dimensional space, a hyperplane is a flat affine subspace of dimension $D − 1$. For instance, in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line. In three dimensions, a hyperplane is a flat two-dimensional subspace—that is, a plane. In $D > 3$ dimensions, it can be hard to visualize a hyperplane, but the notion of a $(D − 1)$-dimensional flat subspace still applies.\n",
    "\n",
    "The mathematical definition of a hyperplane is quite simple. In two dimensions, a hyperplane is defined by the equation,\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 = 0\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\beta_1, \\beta_2$ are parameters. When we say that the above equation “defines” the hyperplane, we mean that any $\\mathbf{x} = [x_1, x_2]^\\top $ satisfies the equation is a point on the hyperplane. Note the equation above is simply the equation of a line, since indeed in two dimensions a hyperplane is a line. In the $D$-dimensional case, the equation defining a hyperplane is\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D = 0\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_D$ are parameters. In this equation, any $\\mathbf{x} = [x_1, x_2, \\ldots, x_D]^\\top$ satisfies the above equation is a point on the hyperplane. If $\\mathbf{x} $ does not satisfy the equation, e.g.\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D > 0\n",
    "$$\n",
    "\n",
    "then $\\mathbf{x} $ is on one side of the hyperplane, and if\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D < 0\n",
    "$$\n",
    "\n",
    "then $\\mathbf{x} $ is on the other side of the hyperplane. So we can think of the hyperplane as dividing $D$-dimensional space into two halves. One can easily determine on which side of the hyperplane a point lies by simply calculating\n",
    "the sign of the left hand side of the hyperplane equation. The following code plots a hyperplane in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hyperplane\n",
    "x = np.arange(-1.5, 1.51, 0.01)\n",
    "y = np.arange(-1.5, 1.51, 0.01)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "zz = np.array([X.ravel(), Y.ravel()]).T\n",
    "Z = zz[:, 0] * 2 + zz[:, 1] * 3 + 1\n",
    "Z[np.where(Z > 0)] = 1\n",
    "Z[np.where(Z <= 0)] = -1\n",
    "Z = Z.reshape(X.shape)\n",
    "plt.contourf(X, Y, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Using a Separating Hyperplane\n",
    "\n",
    "Suppose we have a set of observations $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N$ in $D$-dimensional space\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix} x_{11} \\\\ x_{12} \\\\ \\vdots \\\\ x_{1D} \\end{bmatrix}, \\ldots, \\mathbf{x}_N = \\begin{bmatrix} x_{N1} \\\\ x_{N2} \\\\ \\vdots \\\\ x_{ND} \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and that these observations fall into two classes, $y_1, \\ldots, y_N \\in \\{-1, 1\\}$, where 1 represents one class and -1 the other class. We also have a test observation $\\mathbf{x}^* = [x^*_1, \\ldots, x^*_N]^\\top$ that we would like to classify. We can do so by finding the hyperplane that separates the two classes. Such a hyperplane is known as an separating hyperplane. \n",
    "\n",
    "We can label the observations from the blue class as $ y_i = 1 $ and those from the purple class as $ y_i = −1 $. Then a separating hyperplane has the property that\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_D x_{iD} > 0, \\text{ if } y_i = 1\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_D x_{iD} < 0, \\text{ if } y_i = -1.\n",
    "$$\n",
    "\n",
    "Equivalently, we can write this as\n",
    "\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_D x_{iD}) > 0, \\text{ for } i = 1, \\ldots, N.\n",
    "$$\n",
    "\n",
    "If a separating hyperplane exists, we can use it to construct a very natural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Maximum Margin Classifier\n",
    "\n",
    "In general, if the data can be perfectly separated using a hyperplane, then there will in fact exist an infinite number of such hyperplanes. This is because a given separating hyperplane can usually be shifted a tiny bit up or down, or rotated, without coming into contact with any of the observations.\n",
    "\n",
    "A natural choice is the _maximal margin hyperplane_ (also known as the optimal separating hyperplane), which is the separating hyperplane that is farthest from the training observations. That is, we can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin. The maximal margin hyperplane is the separating hyperplane for which the margin is margin largest—that is, it is the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the maximal margin classifier. We hope that a classifier that has a large margin on the training data will also have a large margin on the test data, and hence will classify the test observations correctly. \n",
    "\n",
    "\n",
    "### Construction of maximum margin classifier\n",
    "\n",
    "```{math}\n",
    ":label: eq:max-margin-classifier\n",
    "\\begin{aligned}\n",
    "& \\max_{\\beta_0, \\beta_1, \\ldots, \\beta_D} M \\\\\n",
    "& \\text{subject to } \\sum_{j=1}^D \\beta_j^2 = 1 ,\\\\\n",
    "& y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_D x_{iD}) \\geq 1, \\text{ for } i = 1, \\ldots, N.\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "where $M$ represents the margin of our hyperplane, and the optimization problem chooses $ \\beta_0, \\beta_1, \\ldots, \\beta_D $ to maximize M. This is exactly the definition of the maximal margin hyperplane! The problem {numref}`max-margin-classifier` can be solved efficiently via quadratic programming, but details of this optimization are outside of the scope of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = np.array([X.ravel(), Y.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = zz[:, 0] * 2 + zz[:, 1] * 3 + 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.1 Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot a classifier with support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc(svc, X, y, h=0.02, pad=0.25):\n",
    "    x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad\n",
    "    y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    print(xx.shape)\n",
    "    print(yy)\n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "    # Support vectors indicated in plot by vertical lines\n",
    "    sv = svc.support_vectors_\n",
    "    plt.scatter(\n",
    "        sv[:, 0], sv[:, 1], c=\"k\", marker=\"x\", s=100, alpha=0.5\n",
    "    )  # , linewidths=1)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.show()\n",
    "    print(\"Number of support vectors: \", svc.support_.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data: 20 observations of 2 features and divide into two classes.\n",
    "np.random.seed(5)\n",
    "X = np.random.randn(20, 2)\n",
    "y = np.repeat([1, -1], 10)\n",
    "\n",
    "X[y == -1] = X[y == -1] + 1\n",
    "plt.scatter(X[:, 0], X[:, 1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifier with linear kernel.\n",
    "svc = SVC(C=1.0, kernel=\"linear\")\n",
    "svc.fit(X, y)\n",
    "\n",
    "plot_svc(svc, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using a smaller cost parameter (C=0.1) the margin is wider, resulting in more support vectors.\n",
    "svc2 = SVC(C=0.1, kernel=\"linear\")\n",
    "svc2.fit(X, y)\n",
    "plot_svc(svc2, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal C parameter by cross-validation\n",
    "tuned_parameters = [{\"C\": [0.001, 0.01, 0.1, 1, 5, 10, 100]}]\n",
    "clf = GridSearchCV(\n",
    "    SVC(kernel=\"linear\"),\n",
    "    tuned_parameters,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "clf.fit(X, y)\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.001 is best according to GridSearchCV.\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating test data\n",
    "np.random.seed(1)\n",
    "X_test = np.random.randn(20, 2)\n",
    "y_test = np.random.choice([-1, 1], 20)\n",
    "X_test[y_test == 1] = X_test[y_test == 1] - 1\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=70, c=y_test, cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc2 : C = 0.1\n",
    "y_pred = svc2.predict(X_test)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred), index=svc.classes_, columns=svc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc3 = SVC(C=0.001, kernel=\"linear\")\n",
    "svc3.fit(X, y)\n",
    "\n",
    "# svc3 : C = 0.001\n",
    "y_pred = svc3.predict(X_test)\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred), index=svc3.classes_, columns=svc3.classes_\n",
    ")\n",
    "# The misclassification is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the test data so that the classes are really seperable with a hyperplane.\n",
    "X_test[y_test == 1] = X_test[y_test == 1] - 1\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=70, c=y_test, cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc4 = SVC(C=10.0, kernel=\"linear\")\n",
    "svc4.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svc4, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the margin. Now there is one misclassification: increased bias, lower variance.\n",
    "svc5 = SVC(C=1, kernel=\"linear\")\n",
    "svc5.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svc5, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.2 Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating test data\n",
    "np.random.seed(8)\n",
    "X = np.random.randn(200, 2)\n",
    "X[:100] = X[:100] + 2\n",
    "X[101:150] = X[101:150] - 2\n",
    "y = np.concatenate([np.repeat(-1, 150), np.repeat(1, 50)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1.0, kernel=\"rbf\", gamma=1)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing C parameter, allowing more flexibility\n",
    "svm2 = SVC(C=100, kernel=\"rbf\", gamma=1.0)\n",
    "svm2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svm2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{\"C\": [0.01, 0.1, 1, 10, 100], \"gamma\": [0.5, 1, 2, 3, 4]}]\n",
    "clf = GridSearchCV(\n",
    "    SVC(kernel=\"rbf\"),\n",
    "    tuned_parameters,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, clf.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15% of test observations misclassified\n",
    "clf.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.3 ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the ROC curves of two models on train/test data. One model is more flexible than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm3 = SVC(C=1, kernel=\"rbf\", gamma=2)\n",
    "svm3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More flexible model\n",
    "svm4 = SVC(C=1, kernel=\"rbf\", gamma=50)\n",
    "svm4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_score3 = svm3.decision_function(X_train)\n",
    "y_train_score4 = svm4.decision_function(X_train)\n",
    "\n",
    "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_train, y_train_score3)\n",
    "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
    "\n",
    "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_train, y_train_score4)\n",
    "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax1.plot(\n",
    "    false_pos_rate3,\n",
    "    true_pos_rate3,\n",
    "    label=\"SVM $\\gamma = 1$ ROC curve (area = %0.2f)\" % roc_auc3,\n",
    "    color=\"b\",\n",
    ")\n",
    "ax1.plot(\n",
    "    false_pos_rate4,\n",
    "    true_pos_rate4,\n",
    "    label=\"SVM $\\gamma = 50$ ROC curve (area = %0.2f)\" % roc_auc4,\n",
    "    color=\"r\",\n",
    ")\n",
    "ax1.set_title(\"Training Data\")\n",
    "\n",
    "y_test_score3 = svm3.decision_function(X_test)\n",
    "y_test_score4 = svm4.decision_function(X_test)\n",
    "\n",
    "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_test, y_test_score3)\n",
    "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
    "\n",
    "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_test, y_test_score4)\n",
    "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)\n",
    "\n",
    "ax2.plot(\n",
    "    false_pos_rate3,\n",
    "    true_pos_rate3,\n",
    "    label=\"SVM $\\gamma = 1$ ROC curve (area = %0.2f)\" % roc_auc3,\n",
    "    color=\"b\",\n",
    ")\n",
    "ax2.plot(\n",
    "    false_pos_rate4,\n",
    "    true_pos_rate4,\n",
    "    label=\"SVM $\\gamma = 50$ ROC curve (area = %0.2f)\" % roc_auc4,\n",
    "    color=\"r\",\n",
    ")\n",
    "ax2.set_title(\"Test Data\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.plot([0, 1], [0, 1], \"k--\")\n",
    "    ax.set_xlim([-0.05, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the more flexible model scores better on training data but worse on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.4 SVM with Multiple Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third class of observations\n",
    "np.random.seed(8)\n",
    "XX = np.vstack([X, np.random.randn(50, 2)])\n",
    "yy = np.hstack([y, np.repeat(0, 50)])\n",
    "XX[yy == 0] = XX[yy == 0] + 4\n",
    "\n",
    "plt.scatter(XX[:, 0], XX[:, 1], s=70, c=yy, cmap=plt.cm.prism)\n",
    "plt.xlabel(\"XX1\")\n",
    "plt.ylabel(\"XX2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm5 = SVC(C=1, kernel=\"rbf\")\n",
    "svm5.fit(XX, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svm5, XX, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.5 Application to Gene Expression Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R, I exported the dataset from package 'ISLR' to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"Data/Khan_xtrain.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "y_train = (\n",
    "    pd.read_csv(\"Data/Khan_ytrain.csv\").drop(\"Unnamed: 0\", axis=1).as_matrix().ravel()\n",
    ")\n",
    "X_test = pd.read_csv(\"Data/Khan_xtest.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "y_test = (\n",
    "    pd.read_csv(\"Data/Khan_ytest.csv\").drop(\"Unnamed: 0\", axis=1).as_matrix().ravel()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train counts\n",
    "pd.Series(y_train).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test counts\n",
    "pd.Series(y_test).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model gives identical results to the svm() of the R package e1071, also based on libsvm library.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "\n",
    "# This model is based on liblinear library and gives 100 score on the test data.\n",
    "# svc = LinearSVC()\n",
    "\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, svc.predict(X_train))\n",
    "cm_df = pd.DataFrame(cm.T, index=svc.classes_, columns=svc.classes_)\n",
    "cm_df.index.name = \"Predicted\"\n",
    "cm_df.columns.name = \"True\"\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, svc.predict(X_test))\n",
    "cm_df = pd.DataFrame(cm.T, index=svc.classes_, columns=svc.classes_)\n",
    "cm_df.index.name = \"Predicted\"\n",
    "cm_df.columns.name = \"True\"\n",
    "print(cm_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pykale')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
