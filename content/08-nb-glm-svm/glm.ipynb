{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalised linear models\n",
    "\n",
    "We use the [Bikeshare dataset](https://github.com/pykale/transparentML/blob/main/data/Bikeshare.csv) to illustrate generalised linear models (glm). The response is bikers, the number of hourly users of a bike sharing program in Washington, DC. This response value is neither qualitative nor quantitative: instead, it takes on non-negative integer values, or counts. We will consider predicting `bikers` using the covariates `mnth` (month of the year), `hr` (hour of the day, from 0 to 23), `workingday` (an indicator variable that equals 1 if it is neither a weekend nor a holiday), `temp` (the normalized temperature, in Celsius), and `weathersit` (a qualitative variable that takes on one of four possible values: clear; misty or cloudy; light rain or light snow; or heavy rain or heavy snow.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.formula.api import ols, poisson\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Bikeshare.csv\"\n",
    "\n",
    "data_df = pd.read_csv(data_url, header=0, index_col=0)\n",
    "data_df[\"mnth\"] = data_df[\"mnth\"].astype(\"category\")\n",
    "data_df[\"hr\"] = data_df[\"hr\"].astype(\"category\")\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression on the `Bikeshare` data\n",
    "\n",
    "To begin, we consider predicting `bikers` using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"bikers ~ mnth + hr + temp + workingday + weathersit\", data_df).fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, for example, that a progression of weather from clear to cloudy results in, on average, 12.89 fewer bikers per hour; however, if the weather progresses further to light rain or snow, then this further results in 53.60 fewer bikers per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = [\"Jan\", \"Feb\", \"March\", \"April\", \"May\", \"June\", \"July\", \"Aug\", \"Sept\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "months = [\n",
    "    \"Jan\",\n",
    "    \"Feb\",\n",
    "    \"March\",\n",
    "    \"May\",\n",
    "    \"June\",\n",
    "    \"July\",\n",
    "    \"Aug\",\n",
    "    \"Sept\",\n",
    "    \"Oct\",\n",
    "    \"Nov\",\n",
    "    \"Dec\",\n",
    "]\n",
    "coef_mnth = [est.params[\"mnth[T.%s]\" % _m] for _m in months]\n",
    "coef_hr = [est.params[\"hr[T.%d]\" % _h] for _h in range(1, 24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.plot(months, coef_mnth, \"bo-\")\n",
    "ax1.set_xlabel(\"Month\")\n",
    "ax2.plot(np.arange(1, 24), coef_hr, \"bo-\")\n",
    "ax2.set_xlabel(\"Hour\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_ylabel(\"Coefficient\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figures display the coefficients associated with `mnth` and `hr`, respectively. We see that bike usage is highest in the spring and fall, and lowest during the winter months. Furthermore, bike usage is greatest around rush hour (9 AM and 6 PM), and lowest overnight. Thus, at first glance, fitting a linear regression model to the `Bikeshare` dataset seems to provide reasonable and intuitive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "y_pred = est.predict(data_df.loc[:, [\"mnth\", \"hr\", \"temp\", \"workingday\", \"weathersit\"]])\n",
    "sns.histplot(y_pred, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But upon more careful inspection, some issues become apparent. For example, as shown in the figure above, 9.6% of the fitted values in the `Bikeshare` data set are negative: that is, the linear regression model predicts a negative number of users during 9.6% of the hours in the data set. This calls into question our ability to perform meaningful predictions on the data, and it also raises concerns about the accuracy of the coefficient estimates, confidence intervals, and other outputs of the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 7))\n",
    "\n",
    "sns.stripplot(data=data_df, x=\"hr\", y=\"bikers\", ax=ax1, alpha=0.1, color=\".2\")\n",
    "sns.boxplot(\n",
    "    data=data_df,\n",
    "    x=\"hr\",\n",
    "    y=\"bikers\",\n",
    "    ax=ax1,\n",
    "    width=0.9,\n",
    "    palette=\"vlag\",\n",
    "    meanline=True,\n",
    "    showmeans=True,\n",
    ")\n",
    "ax1.set_ylabel(\"Number of Bikers\")\n",
    "\n",
    "sns.stripplot(data=data_df, x=\"hr\", y=\"bikers\", ax=ax2, alpha=0.1, color=\".2\")\n",
    "sns.boxplot(\n",
    "    data=data_df,\n",
    "    x=\"hr\",\n",
    "    y=\"bikers\",\n",
    "    ax=ax2,\n",
    "    width=0.9,\n",
    "    palette=\"vlag\",\n",
    "    meanline=True,\n",
    "    showmeans=True,\n",
    ")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_ylabel(\"log(Number of Bikers)\")\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Hour\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, it is reasonable to suspect that when the expected value of bikers is small, the variance of bikers should be small as well. For instance, at 2 AM during a heavy December snow storm, we expect that extremely few people will use a bike, and moreover that there should be little variance associated with the number of users during those conditions. This is borne out in the data: between 1 AM and 4 AM, in December, January, and February, when it is raining, there are 5.05 users, on average, with a standard deviation of 3.73. By contrast, between 7 AM and 10 AM, in April, May, and June, when skies are clear, there are 243.59 users, on average, with a standard deviation of 131.7. The mean-variance relationship is displayed in the left-hand panel of the figure above. This is a major violation of the assumptions of a linear model, which state that $y = \\sum_{j=1}^{D}x_j\\beta_j + \\epsilon$, where $\\epsilon$ is a mean-zero error term with variance $\\sigma^2$ that is constant, and not a function of the covariates. Therefore, the heteroscedasticity of the data calls into question the suitability of a linear regression model.\n",
    "\n",
    "Finally, the response bikers is integer-valued. But under a linear model, $y = \\beta_0 + \\sum_{j=1}^{D}x_j\\beta_j + \\epsilon$, where $\\epsilon$ is a continuous-valued error term. This means that in a linear model, the response $y$ is necessarily continuous-valued (quantitative). Thus, the integer nature of the response bikers suggests that a linear regression model is not entirely satisfactory for this data set.\n",
    "\n",
    "Some of the problems that arise when fitting a linear regression model to the Bikeshare data can be overcome by transforming the response; for instance, we can fit the model\n",
    "\n",
    "$$\n",
    "\\log(y) = \\beta_0 + \\sum_{j=1}^{D}x_j\\beta_j + \\epsilon.\n",
    "$$\n",
    "\n",
    "Transforming the response avoids the possibility of negative predictions, and it overcomes much of the heteroscedasticity in the untransformed data, as is shown in the right-hand panel of the figure above. However, it is not quite a satisfactory solution, since predictions and inference are made in terms of the log of the response, rather than the response. This leads to challenges in interpretation, e.g. ``a one-unit increase in $x_j$ is associated with an increase in the mean of the log of $y$ by an amount $\\beta_j$. Furthermore, a log transformation of the response cannot be applied in settings where the response can take on a value of 0. Thus, while fitting a linear model to a transformation of the response may be an adequate approach for some count-valued data sets, it often leaves something to be desired. We will see in the next section that a Poisson regression model provides a much more natural and elegant approach for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson regression on the `Bikeshare` data\n",
    "\n",
    "To overcome the inadequacies of linear regression for analysing the `Bikeshare` data set, we will make use of an alternative approach, called Poisson regression. Before we can talk about Poisson regression, we must first introduce the Poisson distribution.\n",
    "\n",
    "Suppose that a random variable Y takes on nonnegative integer values, i.e. $y \\in \\{0, 1, 2, \\dots\\}$. If $y$ follows the Poisson distribution, then\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k = 0, 1, 2, \\dots .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = poisson(\"bikers ~ mnth + hr + temp + workingday + weathersit\", data_df).fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_mnth = [est.params[\"mnth[T.%s]\" % _m] for _m in months]\n",
    "coef_hr = [est.params[\"hr[T.%d]\" % _h] for _h in range(1, 24)]\n",
    "\n",
    "# Create plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.plot(months, coef_mnth, \"bo-\")\n",
    "ax1.set_xlabel(\"Month\")\n",
    "ax2.plot(np.arange(1, 24), coef_hr, \"bo-\")\n",
    "ax2.set_xlabel(\"Hour\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    #     ax.legend([\"student\", \"non-student\"], loc=2)\n",
    "    #     ax.set_xlabel(\"Income\")\n",
    "    ax.set_ylabel(\"Coefficient\")\n",
    "#     ax.set_ylim(ymax=1550)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized linear models\n",
    "\n",
    "Common characteristics of generalized linear models:\n",
    "\n",
    "1. Each approach uses predictors $ x_1, x_2, \\dots, x_D $ to predict a response $ y $. We assume that, conditional on $ x_1, x_2,\\dots, x_D $, $ y $ belongs to a certain family of distributions. For linear regression, we typically assume that $ y $ follows a Gaussian or normal distribution. For logistic regression, we assume that $y$ follows a Bernoulli distribution. Finally, for Poisson regression, we assume that $y$ follows a Poisson distribution.\n",
    "2. Each approach models the mean of $y$ as a function of the predictors. In linear regression, the mean of $y$ takes the form\n",
    "    $$\n",
    "    \\mathbb{E}(y|x_1, x_2, \\dots, x_D) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_D.\n",
    "    $$\n",
    "    In logistic regression, the mean of $y$ takes the form\n",
    "    $$\n",
    "    \\mathbb{E}(y|x_1, x_2, \\dots, x_D) = \\mathbb{P}(y=1|x_1, x_2, \\dots, x_D) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_D}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_D}}.\n",
    "    $$\n",
    "    In Poisson regression, the mean of $y$ takes the form\n",
    "    $$\n",
    "    \\mu = e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_D}.\n",
    "    $$\n",
    "\n",
    "The Gaussian, Bernoulli and Poisson distributions are all members of a wider class of distributions, known as the exponential family. Other well known members of this family are the exponential distribution, the Gamma distribution, and the negative binomial distribution. In general, we can perform a regression by modelling the response $y$ as coming from a particular member of the exponential family, and then transforming the mean of the response so that the transformed mean is a linear function of the predictors via (4.42). Any regression approach that follows this very general recipe is known as a generalized linear model (GLM). Thus, linear regression, logistic regression, and Poisson regression are three examples of GLMs. Other examples not covered here include _Gamma regression_ and _negative binomial regression_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "min 3 max 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
