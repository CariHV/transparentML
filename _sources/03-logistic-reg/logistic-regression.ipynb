{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Although having \"regression\" in its name, logistic regression is a classification (rather than regression) algorithm. Instead of modelling the qualitative response $y$ directly, logistic regression models the probability that $y$ belongs to a particular class (category).\n",
    "\n",
    "Watch the 8-minute video below for a visual explanation of logistic regression:\n",
    "\n",
    "```{admonition} Video\n",
    "\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/yIYKR4sgzI8?start=24\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining Logistic Regression, by StatQuest](https://www.youtube.com/embed/yIYKR4sgzI8?start=24)\n",
    "\n",
    "```\n",
    "\n",
    "## Import libraries and load data\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, plot_roc_curve\n",
    "\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv) and inspect the first three rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Default.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Note: factorize() returns two objects: a label array and an array with the unique values. We are only interested in the first object.\n",
    "df[\"default2\"] = df.default.factorize()[0]\n",
    "df[\"student2\"] = df.student.factorize()[0]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models the probability that the response $y$ belongs to a particular class (category) using linear _regression_ via a _logistic_ function, hence the name _logistic regression_. For binary classification, let us use 1 to denote the positive class (e.g. \"Yes\") and 0 to denote the negative class (e.g. \"No\").\n",
    "\n",
    "### Logit function transformation\n",
    "\n",
    "Let $\\pi$ be the probability of a positive outcome \n",
    "\n",
    "$$\n",
    "\\pi = \\mathbb{P}(y=1| x).\n",
    "$$\n",
    "\n",
    "Then the probability of a negative outcome $ \\mathbb{P}(y=0| x) = 1-\\pi $. \n",
    "\n",
    "The [odds](https://en.wikipedia.org/wiki/Odds) of a positive outcome is the ratio of the probability of a positive outcome to the probability of a negative outcome, i.e.\n",
    "\n",
    "$$\n",
    "\\frac{\\pi}{1-\\pi}.\n",
    "$$\n",
    "\n",
    "The log of the odds is called the [logit](https://en.wikipedia.org/wiki/Logit) and the logit function is defined as :\n",
    "\n",
    "$$\n",
    "\\mathrm{logit}(\\pi)=\\log \\frac{\\pi}{1-\\pi}\n",
    "$$ \n",
    "\n",
    "The logit function is a monotonic transformation of the probability $\\pi$. It maps the probability $\\pi$ ranging from 0 to 1 to the real line ranging from $-\\infty$ to $\\infty$. The logit function is also known as the log-odds function.\n",
    "\n",
    "### Logistic function\n",
    "\n",
    "In simple linear regression, we have modelled the relationship between outcome $y$ and feature $x$ with a linear equation:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "This linear regression model is not able to produce a binary outcome for binary classification or a probability (in the range from 0 to 1 only). With the logit function above, we can transform the probability $\\pi$ ranging from 0 to 1 to the logit function $\\mathrm{logit}(\\pi)$ ranging from $-\\infty$ to $\\infty$. Now we can use the linear regression model to model the logit function $\\mathrm{logit}(\\pi)$ instead of the probability $\\pi$ or $y$ directly. The logit function is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{logit}(\\pi)=\\log \\frac{\\pi}{1-\\pi} = \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "The [logistic function](https://en.wikipedia.org/wiki/Logistic_function), also known as the sigmoid function, is the inverse of the logit function. It maps the logit function $\\mathrm{logit}(\\pi)$ ranging from $-\\infty$ to $\\infty$ to the probability $\\pi$ ranging from 0 to 1:\n",
    "\n",
    "$$\n",
    "\\pi = \\mathbb{P}(y = 1 | x) = \\mathrm{logit}^{-1}(\\beta_0 + \\beta_1 x) = \\mathrm{logistic}(\\beta_0 + \\beta_1 x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1)}}.\n",
    "$$\n",
    "\n",
    "<!-- For classification, we cannot use linear regression to model $y$ directly, because $y$ is either. Instead, we can model the logit of the probability of a positive outcome:\n",
    "\n",
    " we prefer probabilities between 0 and 1, so we wrap the right side of the equation into the logistic function. This forces the output to assume only values between 0 and 1 -->\n",
    "\n",
    "\n",
    "<!-- Specifically, instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. \n",
    "\n",
    "$$\n",
    "\\textrm{logistic}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the coefficients to make predictions\n",
    "\n",
    "The coefficients of a logistic regression model can be estimated by [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE). The likelihood function is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} \\mathbb{P}(y_i = 1) \\prod_{i:y_i=0} (1 - \\mathbb{P}(y_i = 1)).\n",
    "$$\n",
    "\n",
    "The mathematical details are beyond the scope of this course. If interested, please read Section 4.3 of the [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/) book for more details of the optimization for logistic regression.\n",
    "\n",
    "To make predictions, taking the classification problem on the [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv), the probability of default given balance predicted by a logistic regression model is\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{default} = \\text{Yes} \\mid \\text{balance}, \\beta_0, \\beta_1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\times \\text{balance})}}.\n",
    "$$\n",
    "\n",
    "### Example explanation of system transparency\n",
    "\n",
    "Let us to see the curve of the logistic function on the [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x=\"balance\", y=\"default2\", logistic=True)\n",
    "plt.ylabel(\"Probability of default\")\n",
    "plt.xlabel(\"Balance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an S-shaped curve with the vertical axis ranging from 0 to 1, indicating the probability of a positive outcome (`default`) with respect to the horizontal axis $x$ (`balance`). \n",
    "\n",
    "In practice, we can use the `scikit-learn` or `statsmodels` package to fit a logistic regression model and make predictions. \n",
    "\n",
    "Let us fit a logistic regression model using the [`scikit-learn` API for logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "X_train = df.balance.values.reshape(-1, 1)\n",
    "X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1, 1)\n",
    "y = df.default2\n",
    "clf.fit(X_train, y)\n",
    "print(clf)\n",
    "print(\"classes: \", clf.classes_)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the fitted logistic regression model and study the system transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    "plt.scatter(X_train, y, color=\"orange\")\n",
    "plt.plot(X_test, prob[:, 1], color=\"lightblue\")\n",
    "\n",
    "plt.hlines(\n",
    "    1,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "plt.hlines(\n",
    "    0,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    0.5,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    -clf.intercept_ / clf.coef_[0],\n",
    "    ymin=plt.gca().yaxis.get_data_interval()[0],\n",
    "    ymax=plt.gca().yaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    (clf.predict_proba(np.asarray(1500).reshape(-1, 1)))[0][1],\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=1500,\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    1500,\n",
    "    ymin=plt.gca().yaxis.get_data_interval()[0],\n",
    "    ymax=(clf.predict_proba(np.asarray(1500).reshape(-1, 1)))[0][1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Probability of default\")\n",
    "plt.xlabel(\"Balance\")\n",
    "plt.yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "plt.xlim(xmin=-100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we learnt a logistic regression model $f(x)$ with two parameters, $\\beta_0$ and $\\beta_1$, from the data, where\n",
    "- $\\beta_0 = -10.65133019 $ \n",
    "- $\\beta_1 = 0.00549892 $ \n",
    "\n",
    "Using these two estimated parameters, we can examine the system logic of the logistic regression model to reveal its system transparency.\n",
    "\n",
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- When `balance`=1500 , the predicted probability of `default` is \n",
    "\n",
    "$$ f(1500) = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 \\times 1500}} = \\frac{1}{1 + e^{-10.65133019 + 0.00549892 \\times 1500}} = 0.08294763. $$\n",
    "\n",
    "- When the probability of `default` 0.5 (the commonly used threshold for binary classification), the corresponding balance can be calculated using the _log odds_ equation above:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "\\ln \\left(\\frac{\\pi}{1-\\pi}\\right) &= \\beta_0 + \\beta_1 \\times x \\\\\n",
    "\\ln \\left(\\frac{0.5}{1-0.5}\\right) &= \\beta_0 + \\beta_1 \\times x \\\\\n",
    "x & = \\frac{- \\beta_0}{\\beta_1} \\\\\n",
    "x & = \\frac{- (-10.65133019)}{0.00549892} \\\\\n",
    "x & = 1936.9858426745614.\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, to produce result `Yes`, i.e. for the probability of `default` $>$ 0.5, the `balance` should be greater than 1936.9858426745614. Likewise, to produce result `No`, i.e. the probability of `default` $<$ 0.5, the `balance` should be less than 1936.9858426745614.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit a logistic regression model using the [`statsmodels` API for logistic regression] next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ balance\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the estimated parameters are the same as those from the `scikit-learn` API. Now let us fit a logistic regression model using `student` status as the input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ student\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient associated with `student` is positive, indicating that the probability of `default` is higher for students than non-students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple logistic regression\n",
    "\n",
    "The logistic regression model can be extended to multiple features. A generalised logistic regression model for an instance $\\mathbf{x} = [x_1, x_2, \\dots, x_D]^\\top$ is\n",
    "\n",
    "$$\n",
    "\\ln \\left( \\frac{\\pi}{1-\\pi} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D.\n",
    "$$\n",
    "\n",
    "Let us fit a multiple logistic regression model using the `scikit-learn` API first.\n",
    "\n",
    "Let us plot the fitted multiple logistic regression model and study the system transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train = df.loc[:, [\"balance\", \"income\", \"student2\"]]\n",
    "y = df.default2\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\", penalty=\"none\", max_iter=1000)\n",
    "clf.fit(X_train, y)\n",
    "print(clf)\n",
    "print(\"classes: \", clf.classes_)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit a multiple logistic regression model using the `statsmodels` API next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ balance + income + student\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar parameters are estimated as using the `scikit-learn` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of logistic regression\n",
    "\n",
    "The evaluation of logistic regression is similar to the evaluation of linear regression. The main difference is that the evaluation metrics are different. For example, the mean squared error (MSE) is not a good metric for classification. Here we introduce two common metrics for classification: accuracy and area under [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (ROC) curve, i.e. AUC (or more precisely, AUROC).\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "The accuracy is the fraction of correct predictions. It is defined as\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{\\text{number of correct predictions}}{\\text{total number of predictions}}.\n",
    "$$\n",
    "\n",
    "Let us compute the training accuracy of the multiple logistic regression model trained using `scikit-learn` API above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "accuarcy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy: \", accuarcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the ROC curve (AUC)\n",
    "\n",
    "The receiver operating characteristic (ROC) curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The TPR and FPR are defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{TPR} &= \\frac{\\text{number of true positives}}{\\text{number of positives}} \\\\\n",
    "\\text{FPR} &= \\frac{\\text{number of false positives}}{\\text{number of negatives}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The area under the ROC curve (AUC) is a popular measure of classifier performance. The AUC is defined as the area under the ROC curve. It is a number between 0 and 1, where 0.5 is the random guess and 1 is the perfect prediction. The AUC is a good metric for evaluating the classification since it considers all possible classification thresholds.\n",
    "\n",
    "Let us plot the ROC curve of the multiple logistic regression model trained using `scikit-learn` API above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = clf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y, y_prob)\n",
    "print(\"AUC: \", roc_auc)\n",
    "\n",
    "clf_disp = plot_roc_curve(clf, X_train, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly the trade-off between TPR and FPR. The AUC is 0.95, indicating that the model is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. Logistic regression is a classification algorithm.\n",
    "\n",
    "        a. True\n",
    "        \n",
    "        b. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**a. True**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. Suppose we have a dataset with 1265 samples. We split the dataset into training and test sets in an 80:20 ratio. We trained a logistic regression model with the train set and tested it with the test set. In the testing, our model predicted 153 samples correctly. Then what is the test accuracy of our trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "0.60\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. If the dataset from Exercise 2 is an equally balanced dataset and the splitting was equal for each class (floor the sample size of both classes). Now, if, from the 153 correctly predicted samples, 100 are from the positive class, what is the True Positive Rate (TPR) value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "TPR = 100/126 = 0.79\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**. All the follwoing exercises will use [Weekly](https://github.com/pykale/transparentML/blob/main/data/Weekly.csv) dataset.\n",
    "\n",
    "Load the dataset, convert the categories to numerical values, and fit a logistic regression model using volume as a predictor and Direction as the response using Scikit-Learn or StatsModel. Find out the fitted model's weight and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Weekly.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "df[\"Direction1\"] = df.Direction.factorize()[0]\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "X_train = df.Lag1.values.reshape(-1, 1)\n",
    "X_test = np.arange(df.Lag1.min(), df.Lag1.max()).reshape(-1, 1)\n",
    "y = df.Direction1\n",
    "clf.fit(X_train, y)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5**. We learned a logistic regression model $f(x)$ with two parameters weight ($\\beta_1$) and bias ($\\beta_0$), where we already have the weight and bias of the model from Exercise-4. Using these two estimated parameters, we can examine the system logic of the logistic regression model to reveal its system transparency. For the probability of Direction > 0.5, what condition should Lag1 follow? Visualize them on the regression plot. Hint: Use the log odds equation from [3.2.2.2](https://pykale.github.io/transparentML/03-logistic-reg/logistic-regression.html#logistic-function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    "plt.scatter(X_train, y, color=\"orange\")\n",
    "plt.plot(X_test, prob[:, 1], color=\"lightblue\")\n",
    "\n",
    "print(-clf.intercept_ / clf.coef_[0])\n",
    "\n",
    "plt.hlines(\n",
    "    1,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "plt.hlines(\n",
    "    0,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    0.5,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    -clf.intercept_ / clf.coef_[0],\n",
    "    ymin=plt.gca().yaxis.get_data_interval()[0],\n",
    "    ymax=plt.gca().yaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "\n",
    "plt.ylabel(\"Probability of Direction\")\n",
    "plt.xlabel(\"Lag1\")\n",
    "plt.show()\n",
    "\n",
    "# For the probability of Direction > 0.5, the Lag1 should be less than 5.34.\n",
    "# Likewise, to produce result No, i.e. the probability of   Direction < 0.5, the Lag1 should be less than 5.34."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6**. Use the fitted logistic regression model from **Exercise-4** and predict the Direction probability when Lag1 = 0.7. Visualize them on the regression plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    "plt.scatter(X_train, y, color=\"orange\")\n",
    "plt.plot(X_test, prob[:, 1], color=\"lightblue\")\n",
    "\n",
    "\n",
    "print(clf.predict_proba(np.asarray(6).reshape(-1, 1))[0][1])\n",
    "\n",
    "plt.hlines(\n",
    "    (clf.predict_proba(np.asarray(6).reshape(-1, 1)))[0][1],\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=6,\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    6,\n",
    "    ymin=plt.gca().yaxis.get_data_interval()[0],\n",
    "    ymax=(clf.predict_proba(np.asarray(6).reshape(-1, 1)))[0][1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Probability of Direction\")\n",
    "plt.xlabel(\"Lag1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7**. Plot the ROC curve and find out the training Accuracy and AUC of the fitted model from **Exercise-4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, plot_roc_curve\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "accuarcy = accuracy_score(y, y_pred)\n",
    "print(\"Training Accuracy: \", accuarcy)\n",
    "\n",
    "y_prob = clf.predict_proba(X_train)[:, 1]\n",
    "roc_auc = roc_auc_score(y, y_prob)\n",
    "print(\"AUC: \", roc_auc)\n",
    "\n",
    "clf_disp = plot_roc_curve(clf, X_train, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8**. Train another logistic regression model using Direction as a response and all the other features as predictors. Print out all the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = df.drop([\"Direction\", \"Direction1\"], axis=1)\n",
    "\n",
    "y = df.Direction1\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\", max_iter=1000)\n",
    "clf.fit(X_train, y)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9**. Plot the ROC curve and find out the Accuracy and AUC metrics of the fitted model from **Exercise-8**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "accuarcy = accuracy_score(y, y_pred)\n",
    "print(\"Training Accuracy: \", accuarcy)\n",
    "\n",
    "y_prob = clf.predict_proba(X_train)[:, 1]\n",
    "roc_auc = roc_auc_score(y, y_prob)\n",
    "print(\"AUC: \", roc_auc)\n",
    "\n",
    "clf_disp = plot_roc_curve(clf, X_train, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10**. Which model performed well between Exercise 4 and Exercise 8 according to Accuracy and AUC metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "The Exercise-8 fitted model performed well according to Accuracy and AUC metrics.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
