{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic regression is not a regression, but a classification algorithm. It models the probabilities for classification problems.\n",
    "\n",
    "## Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import neighbors\n",
    "\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Default.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Note: factorize() returns two objects: a label array and an array with the unique values.\n",
    "# We are only interested in the first object.\n",
    "df[\"default2\"] = df.default.factorize()[0]\n",
    "df[\"student2\"] = df.student.factorize()[0]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a solution for classification. It models the probability that `y` belongs to a particular category rather than modelling this response `y` directly. Specifically, instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. \n",
    "\n",
    "$$\n",
    "\\textrm{logistic}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The step from linear regression to logistic regression is kind of straightforward. In the simple linear regression model, we have modelled the relationship between outcome and features with a linear equation:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1.\n",
    "$$\n",
    "\n",
    "For classification, we prefer probabilities between 0 and 1, so we wrap the right side of the equation into the logistic function. This forces the output to assume only values between 0 and 1\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y = 1 | x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1)}}.\n",
    "$$\n",
    "\n",
    "Run the code cell below to see the curve of the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x=\"balance\", y=\"default2\", logistic=True)\n",
    "plt.ylabel(\"Probability of default\")\n",
    "plt.xlabel(\"Balance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p(x) = \\mathbb{P}(y=1| x)$, resulting in:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "\\frac{1-p(x)}{p(x)} &= e^{-(\\beta_0 + \\beta_1 x_1)} \\\\\n",
    "\\ln \\left( \\frac{1-p(x)}{p(x)} \\right) &= -(\\beta_0 + \\beta_1 x_1) \\\\\n",
    "\\ln \\left( \\frac{p(x)}{1-p(x)} \\right) &= \\beta_0 + \\beta_1 x_1.\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "The left-hand side is called the _log odds_ or _logit_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the coefficients and making predictions\n",
    "\n",
    "The coefficients of a logistic regression model can be estimated by maximum likelihood estimation. The likelihood function is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} \\mathbb{P}(y_i = 1) \\prod_{i:y_i=0} (1 - \\mathbb{P}(y_i = 1)).\n",
    "$$\n",
    "\n",
    "The mathematical details are beyond the scope of this course. Please read Section 4.3 of the [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/) book for more details of the optimization for logistic regression.\n",
    "\n",
    "To make predictions, taking the classification problem of the `Default` data for example, the probability of default given balance predicted by a logistic regression model is\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{default} = \\text{Yes} \\mid \\text{balance}, \\beta_0, \\beta_1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\times \\text{balance})}}.\n",
    "$$\n",
    "\n",
    "In practice, we can use the `scikit-learn` or `statsmodels` package to fit a logistic regression model and making predictions. Run the following code to fit a logistic regression model using the `scikit-learn` and `statsmodels` packages, respectively.\n",
    "\n",
    "**Example of `scikit-learn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "X_train = df.balance.values.reshape(-1, 1)\n",
    "X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1, 1)\n",
    "y = df.default2\n",
    "clf.fit(X_train, y)\n",
    "print(clf)\n",
    "print(\"classes: \", clf.classes_)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    "# Right plot\n",
    "plt.scatter(X_train, y, color=\"orange\")\n",
    "plt.plot(X_test, prob[:, 1], color=\"lightblue\")\n",
    "\n",
    "plt.hlines(\n",
    "    1,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "plt.hlines(\n",
    "    0,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    0.5,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    -clf.intercept_ / clf.coef_[0],\n",
    "    ymin=plt.gca().yaxis.get_data_interval()[0],\n",
    "    ymax=plt.gca().yaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    (clf.predict_proba(np.asarray(1500).reshape(-1, 1)))[0][1],\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=1500,\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    1500,\n",
    "    ymin=plt.gca().yaxis.get_data_interval()[0],\n",
    "    ymax=(clf.predict_proba(np.asarray(1500).reshape(-1, 1)))[0][1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Probability of default\")\n",
    "plt.xlabel(\"Balance\")\n",
    "plt.yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "plt.xlim(xmin=-100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we learnt a logistic regression model $f(x)$ with two parameters, $\\beta_0$ and $\\beta_1$, from the data, where\n",
    "- $\\beta_0 = -10.65133019 $ \n",
    "- $\\beta_1 = 0.00549892 $ \n",
    "\n",
    "Using these two estimated parameters, we can examine the system logic of the logistic regression model to reveal its system transparency.\n",
    "\n",
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- When `balance`=1500 , the predicted probability of `default` is \n",
    "\n",
    "$$ f(1500) = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 \\times 1500}} = \\frac{1}{1 + e^{-10.65133019 + 0.00549892 \\times 1500}} = 0.08294763. $$\n",
    "\n",
    "- When the probability of default is 0.5, the corresponding balance can be calculated using the _log odds_ equation above:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "\\ln \\left(\\frac{p(x)}{1-p(x)}\\right) &= \\beta_0 + \\beta_1 \\times x \\\\\n",
    "\\ln \\left(\\frac{0.5}{1-0.5}\\right) &= \\beta_0 + \\beta_1 \\tims x \\\\\n",
    "x & = \\frac{- \\beta_0}{\\beta_1} \\\\\n",
    "x & = \\frac{- (-10.65133019)}{0.00549892} \\\\\n",
    "x & = 1936.9858426745614.\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, to produce result `Yes`, i.e. the probability of `default` $>$ 0.5, the `balance` should be greater than 1936.9858426745614. By contrast, to produce result `No`, i.e. the probability of `default` $<$ 0.5, the `balance` should be less than 1936.9858426745614.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of `statsmodels`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ balance\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ student\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple logistic regression\n",
    "\n",
    "The logistic regression model can be extended to multiple features. A generalised logistic regression model for an instance $\\mathbf{x} = [x_1, x_2, \\dots, x_D]^\\top$ is\n",
    "\n",
    "$$\n",
    "\\ln \\left( \\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D.\n",
    "$$\n",
    "\n",
    "Run the following code to fit a multiple logistic regression model using the `scikit-learn` and `statsmodels` packages, respectively.\n",
    "\n",
    "**Example of `scikit-learn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train = df.loc[:, [\"balance\", \"income\", \"student2\"]]\n",
    "y = df.default2\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\", penalty=\"none\", max_iter=1000)\n",
    "clf.fit(X_train, y)\n",
    "print(clf)\n",
    "print(\"classes: \", clf.classes_)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of `statsmodels`**, where the learnt parameters are the same as the `scikit-learn` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ balance + income + student\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
