{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification trees\n",
    "\n",
    "Classification trees are very similar to regression trees, except that the target variable is categorical. In regression trees, we used the mean of the target variable in each region as the prediction. In classification trees, we use the most common class in each region as the prediction. Besides the most common class, we are also interesting in the proportion of each class in each region. This is because the proportion of each class in each region is a measure of the **purity** of the region. A region is pure if it contains only one class. A region is impure if it contains multiple classes.\n",
    "\n",
    "## Gini index\n",
    "\n",
    "Although classification accuracy is a good measure for classification performance, a popular cost function (splitting criterion) for classification trees is the [**Gini index**](https://en.wikipedia.org/wiki/Gini_coefficient), a measure of the impurity of a region (not just feature regions in machine learning, but also regions in society often heard in news). This is because classification accuracy is not sufficiently sensitive for building classification trees. \n",
    "\n",
    "The Gini index is defined as:\n",
    "\n",
    "$$\n",
    "G = 1 - \\sum_{c=1}^C p_c^2 = \\sum_{c=1}^C p_c (1 - p_c),\n",
    "$$\n",
    "\n",
    "where $p_c$ is the proportion of class $c$ in the region. The Gini index is zero if the region is pure, and is one if the region is impure. The Gini index is a measure of the probability of misclassification. The Gini index is also known as the **Gini impurity**.\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Another popular cost function (splitting criterion) for classification trees is the [**entropy**](https://en.wikipedia.org/wiki/Entropy_(information_theory)), which is defined as:\n",
    "\n",
    "$$\n",
    "H = - \\sum_{c=1}^C p_c \\log_2 p_c,\n",
    "$$\n",
    "\n",
    "where $p_c$ is the proportion of class $c$ in the region. The entropy is zero if the region is pure, and is one if the region is impure. \n",
    "\n",
    "When building classification trees, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, and the split that produces the lowest cost is chosen. The Gini index and the entropy are very similar, and the Gini index is slightly faster to compute.\n",
    "\n",
    "## Classification trees for heart disease diagnosis\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Heart dataset](https://github.com/pykale/transparentML/blob/main/data/Heart.csv) (click to explore) to predict whether a patient has heart disease or not. The target variable is `AHD`, which is a binary variable that indicates whether a patient has heart disease or not. \n",
    "\n",
    "Load the dataset and _remove the rows with missing values_. Inspect the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_url = \"https://github.com/pykale/transparentML/raw/main/data/Heart.csv\"\n",
    "\n",
    "heart_df = pd.read_csv(heart_url).dropna()\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`factorize` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html) from the `pandas` library to convert categorical variables to numerical variables. This is because the `sklearn` library only accepts numerical variables. `ChestPain` is a categorical variable that indicates the type of chest pain. `Thal` is a categorical variable that indicates the type of thalassemia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.ChestPain = pd.factorize(heart_df.ChestPain)[0]\n",
    "heart_df.Thal = pd.factorize(heart_df.Thal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the feature matrix `X` and the target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_df.drop(\"AHD\", axis=1)\n",
    "y = pd.factorize(heart_df.AHD)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a classification tree to the data using the `DecisionTreeClassifier` class from the `sklearn.tree` module. Set the `max_leaf_nodes` parameter to 6. This will limit the number of leaf nodes to 6. Set the `max_features` parameter to 3. This will limit the number of features to consider when looking for the best split to 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_leaf_nodes=6, max_features=3)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of the classification tree on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the classification tree using the `plot_tree` function from the `sklearn.tree` module. Set the `filled` parameter to `True` to colour the nodes in the tree according to the majority class in each region. Set the `feature_names` parameter to the list of feature names. Set the `class_names` parameter to the list of class names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    clf, filled=True, feature_names=X.columns, class_names=[\"No\", \"Yes\"], fontsize=14\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "min 3 max 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
